---
title: "Planning by Numbers Assignment 3 - CATEGORICAL DATA MODELS"
author: "Oliver Atwood & Charlie Townsley"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: flatly
    highlight: kate
    code_folding: hide
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
rm(list=ls())
```

```{r colors, include=FALSE}
#set colors
palette <- c("#a02300", "#0059c7", "#6496b4", "#E1B93C")
#red and gold are accent
#blue is chart
#dark blue is outline
```


# Part A


```{r load packages and data, warning = FALSE, message = FALSE}
#run every session
library(tidyverse)
library(dplyr)
library(ggplot2)
library(caret)
library(MASS)
library(corrplot)
library(stargazer)
library(broom)
library(magrittr)
library(raster)
library(sp)
library(mlogit)
```

## A.1 Read the data into R. Run descriptive statistics as necessary.
```{r, warning = FALSE, message = FALSE}
Dat_A <- read_csv("https://raw.githubusercontent.com/c-townsley/pbn_a3_repo/main/Data/Chester_Urban_Growth.csv")
```

```{r summarize data, warning = FALSE, message = FALSE}
summary(Dat_A)
```

<br>

## A.2 Create New Variable for Urban Land Conversion
>Create a new binary (0/1) variable, [CHNG_URB] indicating those raster cells that were
either farmland or pasture land or forest in 1992, and which converted to urban uses by
2001. Indicate these cells with a 1. All other cells should be coded to 0.

```{r Land Use Change Characterization}
Dat_A <- Dat_A %>% 
  mutate(CHNG_URB = ifelse(FARM92 == 1 | PASTURE92 == 1 | FOREST92 == 1 & URBAN01 == 1, 1, 0))

```
<br>

## A.3 Build a binomial logit model to identify drivers of urban land conversion
>Build a best “lean & mean” binomial logit model that identifies the determinants of
agricultural/pasture/forest land-to-urban land use change between 1992 and 2001 (This
is the 0/1 variable created in step 2). Use whatever independent variables you think
appropriate, and be sure to explain your logic and model development steps.

Test correlations with CHNG_URB
```{r Correlations with Urban Land Use Change}
print("SLOPE")
cor(Dat_A$CHNG_URB, Dat_A$SLOPE, use="complete.obs", method="pearson")
print("FOURLNE300")
cor(Dat_A$CHNG_URB, Dat_A$FOURLNE300, use="complete.obs", method="pearson")
print("INTERST800")
cor(Dat_A$CHNG_URB, Dat_A$INTERST800, use="complete.obs", method="pearson")
print("REGRAIL300")
cor(Dat_A$CHNG_URB, Dat_A$REGRAIL300, use="complete.obs", method="pearson")
print("PARKS500M")
cor(Dat_A$CHNG_URB, Dat_A$PARKS500M, use="complete.obs", method="pearson")
print("WATER100")
cor(Dat_A$CHNG_URB, Dat_A$WATER100, use="complete.obs", method="pearson")
print("CITBORO_10")
cor(Dat_A$CHNG_URB, Dat_A$CITBORO_10, use="complete.obs", method="pearson")
print("DIST_WATER")
cor(Dat_A$CHNG_URB, Dat_A$DIST_WATER, use="complete.obs", method="pearson")
print("DIST_RAILS")
cor(Dat_A$CHNG_URB, Dat_A$DIST_RAILS, use="complete.obs", method="pearson")
print("DIST_REGRA")
cor(Dat_A$CHNG_URB, Dat_A$DIST_REGRA, use="complete.obs", method="pearson")
print("DIST_PASSR")
cor(Dat_A$CHNG_URB, Dat_A$DIST_PASSR, use="complete.obs", method="pearson")
print("DIST_4LNE_")
cor(Dat_A$CHNG_URB, Dat_A$DIST_4LNE_, use="complete.obs", method="pearson")
print("DIST_INTER")
cor(Dat_A$CHNG_URB, Dat_A$DIST_INTER, use="complete.obs", method="pearson")
print("DIST_PARKS")
cor(Dat_A$CHNG_URB, Dat_A$DIST_PARKS, use="complete.obs", method="pearson")
print("PAL_WETLND")
cor(Dat_A$CHNG_URB, Dat_A$PAL_WETLND, use="complete.obs", method="pearson")
print("POPDEN90")
cor(Dat_A$CHNG_URB, Dat_A$POPDEN90, use="complete.obs", method="pearson")
print("MEDINC90")
cor(Dat_A$CHNG_URB, Dat_A$MEDINC90, use="complete.obs", method="pearson")
print("MEDHSEVAL_")
cor(Dat_A$CHNG_URB, Dat_A$MEDHSEVAL_, use="complete.obs", method="pearson")
print("PCT_WHITE_")
cor(Dat_A$CHNG_URB, Dat_A$PCT_WHITE_, use="complete.obs", method="pearson")
print("PCT_SFHOME")
cor(Dat_A$CHNG_URB, Dat_A$PCT_SFHOME, use="complete.obs", method="pearson")
print("PCT_POV_90")
cor(Dat_A$CHNG_URB, Dat_A$PCT_POV_90, use="complete.obs", method="pearson")
print("PCT_HSB_19")
cor(Dat_A$CHNG_URB, Dat_A$PCT_HSB_19, use="complete.obs", method="pearson")
print("PCT_COLGRD")
cor(Dat_A$CHNG_URB, Dat_A$PCT_COLGRD, use="complete.obs", method="pearson")
```
The variables in order of correlation with CHNG_URB are as follows:
"DIST_INTER" 0.3188977
"DIST_REGRA" 0.2908341
"DIST_RAILS" 0.290569
"SLOPE" -0.2689929
"DIST_PARKS" 0.2504532
"REGRAIL300" -0.2117686
"INTERST800" -0.2075421
"MEDHSEVAL_" -0.1595283
"MEDINC90" -0.1479734
"PCT_POV_90" 0.115068
"PCT_COLGRD" 0.115068
"DIST_WATER" -0.09334977
"PCT_WHITE_" -0.08178655
"DIST_PASSR" 0.0765004
"PARKS500M" -0.07496059
"PCT_HSB_19" 0.07437706
"POPDEN90" -0.07412875
"DIST_4LNE_" 0.07289739
"FOURLNE300" -0.05593594
"PCT_SFHOME" -0.0524793
"CITBORO_10" -0.03061511
"WATER100" -0.01852956
"PAL_WETLND" -0.007407224

Let's test the top 10 variables most correlated with CHNG_URB for colinearity using a cor plot.
```{r correlation matrix, fig.width=7, fig.height=5, fig.width=8}
#make matrix of variables we're testing
mod_vars <- Dat_A %>% 
  dplyr::select(CHNG_URB, DIST_INTER, DIST_REGRA, DIST_RAILS, SLOPE, DIST_PARKS, REGRAIL300, INTERST800, MEDHSEVAL_, MEDINC90, PCT_POV_90)

#compute correlation matrix
cormatrix <- cor(mod_vars) %>% 
  round(., 2)

#plot a correlogram
corrplot(cormatrix, method = "circle", type = "lower", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

This plot shows that the following variables exhibit correlation with each other, meaning that their inclusion in the model is mutually exclusive.

DIST_INTER & DIST_REGRA & DIST_RAILS
DIST_INTER & INTERST800
MEDHSEVAL_ & MEDINC09

```{r Binomial Logit Model 1}
mod1 <- glm (CHNG_URB ~ DIST_REGRA + SLOPE + DIST_PARKS + REGRAIL300 + INTERST800 + MEDHSEVAL_ + PCT_POV_90, data=Dat_A, family = binomial)
summary(mod1)
```
PCT_POV_90 appears to be insignificant, so let's try removing it from model 2
```{r Binomial Logit Model 2}
mod2<- glm (CHNG_URB ~ DIST_REGRA + SLOPE + DIST_PARKS + REGRAIL300 + INTERST800 + MEDHSEVAL_, data=Dat_A, family = binomial)
summary(mod2)
```
When we remove PCT_POV_90, the AIC decreases by 0.2. INTERST800 also appears to be insignificant, so what happens if we remove that too?

```{r Binomial Logit Model 3}
mod3<- glm (CHNG_URB ~ DIST_REGRA + SLOPE + DIST_PARKS + REGRAIL300 + MEDHSEVAL_, data=Dat_A, family = binomial)
summary(mod3)
```
Now the AIC is back to where it was in the first model. Also, the residual deviance is 0.2 higher than model 2.

Since DIST_INTER is colinear with DIST_REGRA, DIST_RAILS, and INTERST800, it was not included in the first version of the model, so let's try a model with DIST_INTER.

```{r Binomial Logit Model 4}
mod4<- glm (CHNG_URB ~ DIST_INTER + SLOPE + DIST_PARKS + REGRAIL300 + MEDHSEVAL_, data=Dat_A, family = binomial)
summary(mod4)
```
The AIC increased on this model compared to the previous ones, so it seems like model 2 is the way to go.

```{r Calculate Prediction Accuracy}
#model1
#calc the predicted probabilities based on the model
pred <- as.data.frame(fitted(mod1))
pred <- rename(pred, "prob" = "fitted(mod1)")
pred <- mutate(pred, "binary" = ifelse(prob < 0.5, 0, 1))
#append column to original data frame
Dat_A$binary <- pred$binary
head(Dat_A)
#calculate accuracy rate
mod1_Acc <- (sum(Dat_A$URBAN01 == 1 & Dat_A$binary == 1) + sum(Dat_A$URBAN01 == 0 & Dat_A$binary == 0)) / nrow(Dat_A)

#model2
#calc the predicted probabilities based on the model
pred <- as.data.frame(fitted(mod2))
pred <- rename(pred, "prob" = "fitted(mod2)")
pred <- mutate(pred, "binary" = ifelse(prob < 0.5, 0, 1))
#append column to original data frame
Dat_A$binary <- pred$binary
head(Dat_A)
#calculate accuracy rate
mod2_Acc <- (sum(Dat_A$URBAN01 == 1 & Dat_A$binary == 1) + sum(Dat_A$URBAN01 == 0 & Dat_A$binary == 0)) / nrow(Dat_A)

#model3 
#calc the predicted probabilities based on the model
pred <- as.data.frame(fitted(mod3))
pred <- rename(pred, "prob" = "fitted(mod3)")
pred <- mutate(pred, "binary" = ifelse(prob < 0.5, 0, 1))
#append column to original data frame
Dat_A$binary <- pred$binary
head(Dat_A)
#calculate accuracy rate
mod3_Acc <- (sum(Dat_A$URBAN01 == 1 & Dat_A$binary == 1) + sum(Dat_A$URBAN01 == 0 & Dat_A$binary == 0)) / nrow(Dat_A)

#model4
#calc the predicted probabilities based on the model
pred <- as.data.frame(fitted(mod4))
pred <- rename(pred, "prob" = "fitted(mod4)")
pred <- mutate(pred, "binary" = ifelse(prob < 0.5, 0, 1))
#append column to original data frame
Dat_A$binary <- pred$binary
head(Dat_A)
#calculate accuracy rate
mod4_Acc <- (sum(Dat_A$URBAN01 == 1 & Dat_A$binary == 1) + sum(Dat_A$URBAN01 == 0 & Dat_A$binary == 0)) / nrow(Dat_A)

print("model 1 Accuracy")
mod1_Acc
print("model 2 Accuracy")
mod2_Acc
print("model 3 Accuracy")
mod3_Acc
print("model 4 Accuracy")
mod4_Acc
```
<br>

Let's try this another way. What happens if we let the machine do the work of sorting out which variables to include in the model?
```{r ML_Model1}
Dat_B <- Dat_A %>% dplyr::select(-CHESCO, -X, -Y, -PCT_COLGRD, -binary, -FARM92, -PASTURE92, -FOREST92, -URBAN01, -URBAN92)

set.seed(3456)
trainIndex <- createDataPartition(Dat_A$CHNG_URB, p = .70, list = FALSE, times = 1)

Dat_B_Train <- Dat_B[ trainIndex,]
Dat_B_Test  <- Dat_B[-trainIndex,]

ML_Model1 <- glm(CHNG_URB ~ ., family="binomial"(link="logit"), data = Dat_B_Train)
summary(ML_Model1)
```

Train another model, this time removing the variables we know are colinear from our corr plot.
```{r ML_Model2}
Dat_C <- Dat_B %>% dplyr::select(-DIST_REGRA, -DIST_RAILS, -INTERST800, -MEDINC90)

set.seed(3456)
trainIndex2 <- createDataPartition(Dat_C$DIST_INTER, p = .70, list = FALSE, times = 1)

Dat_C_Train <- Dat_C[ trainIndex2,]
Dat_C_Test  <- Dat_C[-trainIndex2,]

ML_Model2 <- glm(CHNG_URB ~ ., family="binomial"(link="logit"), data = Dat_C_Train)
summary(ML_Model2)
```

The AIC increased on Model2 relative to Model1...

Train a third model, removing variables with poor performance indicated by high p-values, along with the intercept.
```{r ML_Model3}
Dat_D <- Dat_C %>% dplyr::select(-CITBORO_10, -DIST_PASSR, -DIST_4LNE_, -PAL_WETLND, -PCT_WHITE_, -PCT_SFHOME)

set.seed(3456)
trainIndex3 <- createDataPartition(Dat_D$DIST_INTER, p = .70, list = FALSE, times = 1)

Dat_D_Train <- Dat_D[ trainIndex3,]
Dat_D_Test  <- Dat_D[-trainIndex3,]

ML_Model3 <- glm(CHNG_URB ~ ., family="binomial"(link="logit"), data = Dat_D_Train)
summary(ML_Model3)
```

Train a fourth model, removing low-performing variables in the previous model.
```{r ML_Model4}
Dat_E <- Dat_D %>% dplyr::select(-MEDHSEVAL_, -PCT_HSB_19)

set.seed(3456)
trainIndex4 <- createDataPartition(Dat_E$DIST_INTER, p = .70, list = FALSE, times = 1)

Dat_E_Train <- Dat_E[ trainIndex4,]
Dat_E_Test  <- Dat_E[-trainIndex4,]

ML_Model4 <- glm(CHNG_URB ~ ., family="binomial"(link="logit"), data = Dat_E_Train)
summary(ML_Model4)
```

Train a fifth model, removing the lowest-performing two variables.
```{r ML_Model5}
Dat_F <- Dat_E %>% dplyr::select(-PARKS500M, -POPDEN90)

set.seed(3456)
trainIndex5 <- createDataPartition(Dat_F$DIST_INTER, p = .70, list = FALSE, times = 1)

Dat_F_Train <- Dat_F[ trainIndex5,]
Dat_F_Test  <- Dat_F[-trainIndex5,]

ML_Model5 <- glm(CHNG_URB ~ ., family="binomial"(link="logit"), data = Dat_F_Train)
summary(ML_Model5)
```

Train a sixth model, removing the lowest-performing two variables.
```{r ML_Model6}
Dat_G <- Dat_F %>% dplyr::select(-RAILSTN100, -PCT_POV_90)

set.seed(3456)
trainIndex6 <- createDataPartition(Dat_G$DIST_INTER, p = .70, list = FALSE, times = 1)

Dat_G_Train <- Dat_G[ trainIndex6,]
Dat_G_Test  <- Dat_G[-trainIndex6,]

ML_Model6 <- glm(CHNG_URB ~ ., family="binomial"(link="logit"), data = Dat_G_Train)
summary(ML_Model6)
```

Train a seventh model, removing the lowest-performing three variables.
```{r ML_Model7}
Dat_H <- Dat_G %>% dplyr::select(-FOURLNE300, -WATER100, -DIST_WATER)

set.seed(3456)
trainIndex7 <- createDataPartition(Dat_H$DIST_INTER, p = .70, list = FALSE, times = 1)

Dat_H_Train <- Dat_H[ trainIndex7,]
Dat_H_Test  <- Dat_H[-trainIndex7,]

ML_Model7 <- glm(CHNG_URB ~ ., family="binomial"(link="logit"), data = Dat_H_Train)
summary(ML_Model7)
```

Compare Results of ALL Models

```{r confusion matricies, message = FALSE, warning = FALSE}
###Model 1
classProbs <- predict(ML_Model1, Dat_B_Test, type="response")

testProbs <- data.frame(obs = as.numeric(Dat_B_Test$CHNG_URB), pred = classProbs)

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

CM_Model1 <- caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")

Model1_Acc <- CM_Model1$overall["Accuracy"]

###Model 2
classProbs <- predict(ML_Model2, Dat_C_Test, type="response")

testProbs <- data.frame(obs = as.numeric(Dat_C_Test$CHNG_URB), pred = classProbs)

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

CM_Model2 <- caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")

Model2_Acc <- CM_Model2$overall["Accuracy"]

###Model 3
classProbs <- predict(ML_Model3, Dat_D_Test, type="response")

testProbs <- data.frame(obs = as.numeric(Dat_D_Test$CHNG_URB), pred = classProbs)

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

CM_Model3 <- caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")

Model3_Acc <- CM_Model3$overall["Accuracy"]

###Model 4
classProbs <- predict(ML_Model4, Dat_E_Test, type="response")

testProbs <- data.frame(obs = as.numeric(Dat_E_Test$CHNG_URB), pred = classProbs)

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

CM_Model4 <- caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")

Model4_Acc <- CM_Model4$overall["Accuracy"]

###Model 5
classProbs <- predict(ML_Model5, Dat_F_Test, type="response")

testProbs <- data.frame(obs = as.numeric(Dat_F_Test$CHNG_URB), pred = classProbs)

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

CM_Model5 <- caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")

Model5_Acc <- CM_Model5$overall["Accuracy"]

###Model 6
classProbs <- predict(ML_Model6, Dat_E_Test, type="response")

testProbs <- data.frame(obs = as.numeric(Dat_E_Test$CHNG_URB), pred = classProbs)

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

CM_Model6 <- caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")

Model6_Acc <- CM_Model6$overall["Accuracy"]

###Model 7
classProbs <- predict(ML_Model7, Dat_F_Test, type="response")

testProbs <- data.frame(obs = as.numeric(Dat_F_Test$CHNG_URB), pred = classProbs)

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

CM_Model7 <- caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")

Model7_Acc <- CM_Model7$overall["Accuracy"]

###Summarize Accuracy
print('Model 1')
Model1_Acc
print('Model 2')
Model2_Acc
print('Model 3')
Model3_Acc
print('Model 4')
Model4_Acc
print('Model 5')
Model5_Acc
print('Model 6')
Model6_Acc
print('Model 7')
Model7_Acc
```
From this summary of accuracy tests, it appears that Model 1 is the most accurate, at ~68%. However, the decrease in accuracy incurred through omission of variables with each successive model generated seems to be worth it, with our 'leanest and meanest' model using only four variables to predict land conversion to urban with ~65% accuracy.


<br>

## A.4
>Explain the results of your best model in a few paragraphs for non-statisticians. How
well does your model fit the data? Which factors are most important in explaining land
use change in Chester County from 1992-2001? How do you know? Do you have any
ideas for improving the performance of your model?

Out of the 23 variables in this dataset, Model 7 uses only four variables to model land use change in Chester County from 1992-2001 with 65% accuracy. According to this model, the four most important factors in explaining land use change are Slope, Land within 300 meters of SEPTA regional rail lines, Distance to Interstate, and distance to parks.

Interpreting the results of the coefficients:
The estimate for the SLOPE variable is -0.3121, which means that for every one-unit increase in slope (percent), the log-odds of a pixel changing to urban (i.e., CHNG_URB being 1) between 1992 and 2001 decreases by 0.3121, holding other variables constant.

To make this more interpretable, we can convert the log-odds to odds by taking the exponent of the coefficient: exp(-0.3121) ≈ 0.732. This means that for every one-unit increase in slope, the odds of a pixel changing to urban between 1992 and 2001 are approximately 73.2% of the odds for the previous slope value, keeping all other predictors constant in the model. Put another way, all else equal, the odds of a given pixel changing to urban decrease by 26.8% for every 1% increase in slope. The simplest interpretation of this coefficient is that an increase in slope is associated with a decreased likelihood of a pixel becoming urban.

For the rest of the values, let's keep it simple:

REGRAIL300 (binary) has a coefficient of -1.016, meaning that a cell being located within 300 meters of a SEPTA regional rail line, is associated with a decreased likelihood of a pixel becoming urban.

DIST_INTER (meters) has a coefficient of .00003284, meaning that the further a pixel is from the interstate, the more likely it is to become urban.
This is likely due to the fact that areas closest to the interstate are already urban.

DIST_PARKS (meters) has a coefficient of 0.00009018 meaning that the further a pixel is from a park, the more likely it is to become urban.
This is likely due to the same phenomenon, since parks tend to be located in urban areas and also parks themselves are unlikely to urbanize.


<br>

## A.5
>Include a few plots or tables that show the probabilities versus changes in key variables
in your model.

```{r predict and plot}

####Plot Predicted Change in probability of Urban Land Conversion based on Slope

#Scenario1: Slope by >300m from rail (0), 12530m (median) distance from interstate, 5161m (median) distance from parks
newdat_gg<-data.frame(matrix(ncol = 4, nrow = nrow(Dat_H)))
colnames(newdat_gg)<- c("SLOPE", "REGRAIL300", "DIST_INTER", "DIST_PARKS")
newdat_gg$SLOPE <- Dat_H$SLOPE
newdat_gg$REGRAIL300 <- 0
newdat_gg$DIST_INTER <- 12530
newdat_gg$DIST_PARKS <- 5161

#Scenario2: Slope by <300m from rail (1), 5590m (Q1) distance from interstate, 2500m (Q1) distance from parks
newdat_gg_1<-data.frame(matrix(ncol = 4, nrow = nrow(Dat_H)))
colnames(newdat_gg_1)<- c("SLOPE", "REGRAIL300", "DIST_INTER", "DIST_PARKS")
newdat_gg_1$SLOPE <- Dat_H$SLOPE
newdat_gg_1$REGRAIL300 <- 1
newdat_gg_1$DIST_INTER <- 5590
newdat_gg_1$DIST_PARKS <- 2500

#Scenario3: Slope by >300m from rail (0), 24076m (Q3) distance from interstate, 7106m (Q3) distance from parks
newdat_gg_2<-data.frame(matrix(ncol = 4, nrow = nrow(Dat_H)))
colnames(newdat_gg_2)<- c("SLOPE", "REGRAIL300", "DIST_INTER", "DIST_PARKS")
newdat_gg_2$SLOPE <- Dat_H$SLOPE
newdat_gg_2$REGRAIL300 <- 1
newdat_gg_2$DIST_INTER <- 24076
newdat_gg_2$DIST_PARKS <- 7106


pred_dat<- data.frame(matrix(ncol = 4, nrow = nrow(Dat_H)))
colnames(pred_dat)<- c("SLOPE", "Pred_SlopexNoRail_MedDInt_MedDPark", "Pred_SlopexRail_LowDInt_LowDPark", "Pred_SlopexNoRail_HiDInt_HiDPark")
pred_dat$SLOPE<-Dat_H$SLOPE
pred_dat$Pred_SlopexNoRail_MedDInt_MedDPark<- predict(ML_Model7, newdat_gg, type="response")
pred_dat$Pred_SlopexRail_LowDInt_LowDPark<- predict(ML_Model7, newdat_gg_1, type="response")
pred_dat$Pred_SlopexNoRail_HiDInt_HiDPark<- predict(ML_Model7, newdat_gg_2, type="response")

dat <- gather(pred_dat, -SLOPE, key = "Scenario", value = "value")

ggplot(dat, aes(x = SLOPE, y = value, colour = Scenario)) + 
        geom_line() + ylim(0,1) +
        xlab("Slope") + ylab("Predicted Probability of Urban Land Use Conversion")
```

<br>

# Part B
>Charlie
>How do a person’s household, demographic, and trip characteristics
affect their choice of transportation mode to work? To answer this
question, we will use data from the DVRPC HHTS.

```{r part B data, warning = FALSE, message = FALSE}
rm(list=ls())

per_dat <- read_csv("https://raw.githubusercontent.com/c-townsley/pbn_a3_repo/main/Data/HH%20Travel%20Survey/per_pub.csv")

trip_dat <- read_csv("https://raw.githubusercontent.com/c-townsley/pbn_a3_repo/main/Data/HH%20Travel%20Survey/trip_pub.csv")

hh_dat <- read_csv("https://raw.githubusercontent.com/c-townsley/pbn_a3_repo/main/Data/HH%20Travel%20Survey/hh_pub_CSV.csv", col_names = TRUE)

head(per_dat)
head(trip_dat)
head(hh_dat)
```

<br>

# B.1 Driving Alone to Work vs. Other Modes

In this section, we develop a binomial LOGIT model to explain whether a commuter drove to work or used another mode.

## B.1A Data Exploration and Processing

```{r process per_dat and hh_dat}
# Develop a dataset with all relevant variables 

# Want to look at the person_dat, but also want to include income, which is in the HH table. 
# matching the datasets on HH ID number, which is the variable "SAMPN"
per_hh_dat <- merge(per_dat, hh_dat,
                    by.x = "SAMPN", #per_dat is the x
                    by.y = "SAMPN", #hh_dat is the y
                    all.x = TRUE,   #taking every row of X and finding the matching Y
                    all.y = FALSE,  #Exclude rows in hh_dat that don't have a matching value in per_dat
                    sort = FALSE)
head(per_hh_dat)

#select variables of interest
per_hh_clean <- per_hh_dat %>% 
  dplyr::select(SAMPN, PERNO, GEND, AGE, INCOM, W1CPK, W1EPK, TRNST, VTRAN, TRNCT, WSTIM, WETIM)

#combine SAMPN and PERNO
per_hh_clean$SAMPN_PER <- paste0(per_hh_clean$SAMPN, per_hh_clean$PERNO) 

#remove NAs
per_hh_clean$INCOM[which(is.na(per_hh_clean$INCOM))]<-0
per_hh_clean$TRNCT[which(is.na(per_hh_clean$TRNCT))] <- 0

summary(per_hh_clean)
glimpse(per_hh_clean)
```

```{r explore transit mode data}
head(trip_dat,10)
hist(trip_dat$TRAN1)
hist(trip_dat$TRAN2)
hist(trip_dat$TRAN3)
```


```{r B.1 trip data cleaning}
#grab anyone who drives alone to work on their first one-way work trip of the day
trip_dat$drivealone[(trip_dat$TRAN1==21 | trip_dat$TRAN2==21 | 
                       trip_dat$TRAN3==21| trip_dat$TRAN4==21)] <-1
trip_dat$drivealone[which(is.na(trip_dat$drivealone))] <-0 #Remove NAs

#now let's pick out that first work trip
CommuteTripsClean <- trip_dat[which(trip_dat$Dest_PTYE == 2), ]

#let's merge in on person number and SAMPN
#we have to merge together the SAMPN and the person number 
#There is an X variable in the data frame already.
#We can replicate this column by concatenating SAMPB and PERNO
CommuteTripsClean$SAMPN_PER <- paste0(CommuteTripsClean$SAMPN, CommuteTripsClean$PERNO)

#grab just the first work trip
CommuteTripsClean <- subset(CommuteTripsClean, !duplicated(SAMPN_PER))

#Retain just the columns we need from the trip data. 
CommuteTripsClean2 <- CommuteTripsClean %>% 
  dplyr::select(SAMPN, PERNO, SAMPN_PER, TOLLA, TOLL, PARKC, PARKU, TRPDUR, drivealone)

#remove NAs
CommuteTripsClean2$TOLLA[which(is.na(CommuteTripsClean2$TOLLA))]<-0
CommuteTripsClean2$TOLL[which(is.na(CommuteTripsClean2$TOLL))]<-0
CommuteTripsClean2$PARKU[which(is.na(CommuteTripsClean2$PARKU))]<-0
CommuteTripsClean2$PARKC[which(is.na(CommuteTripsClean2$PARKC))]<-0

#remove trips with a duration of zero
CommuteTripsClean2 <- CommuteTripsClean2 %>% 
  filter(TRPDUR != 0)

glimpse(CommuteTripsClean2)
summary(CommuteTripsClean2)
```

```{r merge CommuteTripsClean2 with per_hh_clean}
#Merge CommuteTripsClean2 with per_hh_clean via SAMPN_PER
CommuteTripsPerson <- merge(CommuteTripsClean2, 
                            per_hh_clean,
                            by.x = "SAMPN_PER", 
                            by.y = "SAMPN_PER", 
                            all.x = TRUE, 
                            all.y = FALSE, 
                            sort = FALSE)

head(CommuteTripsPerson)
hist(CommuteTripsPerson$drivealone)
```

<br>

## B.1B Variable Selection

>Develop the best (“leanest and meanest” and unbiased) binomial LOGIT model you
can explaining whether a commuter drove to work or used another mode. Use any
and all independent variables you think appropriate, including dummy variables.

>From megan: don't include variables that have a lot of zeros
Ok to do drive alone instead of all driving. Just say how we define it

The binomial Dependent Variable for our model is `drivealone`: Which is whether the person alone drove to work vs. got to work any other way.

Independent variables of interest:
* `GEND`: Person X - Gender
* `AGE`: Person X - Age 
* `INCOM`: total 1999 annual household income 
* `TRPDUR`: trip duration
* `TOLL`: presence of toll (Y/N)
* `TOLLA`: toll amount
* `PARKC`: parking cost ($)
* `PARKU`: parking cost unit (per hour, day, week, month, or other)
* `TRNCT`: Cost of Using Transit (dollar figure)

> Others to maybe add later from TRIP_PUB:
SUBTR     Boarding Station
ACCESS    Access to Boarding Station
PAY1O     First Transit Payment - Other
FARE1     Fare Amount 1 
TRFR      Transfers
PAY2      Second Transit Payment 
PAY2O     Second Transit Payment - Other 
FARE2     Fare Amount 2    
PAY3      Third Transit Payment    
PAY3O     Third Transit Payment - Other 
FARE3     Fare Amount 3 
EGRESS    Egress From Exit Station
OEGRES    Other Egress 

> Other to maybe add from per_hh_dat
W1CPK     Cost to Park Vehicle at Work (dollar figure)  
W1EPK     Employer Subsidize Parking 
TRNST     Employer Subsidize Transit  
VTRAN     Use Transit Subsidy
TRNCT     Cost of Using Transit (dollar figure)
WSTIM     Work Start Time
WETIM     Work End Time 

The values for `GEND`, `TOLL`, and `PARKU` are categorical, so we need to turn them into binary dummy variables to be able to use them in our models.

```{r B1 Dummy Vars}
#Create dummy variables from GEND, TOLL, and PARKU
CommuteTripsPerson <- CommuteTripsPerson %>% 
  mutate(GEND_M = ifelse(GEND == 1, 1, 0)) %>% #create binary for male drivers
  mutate(GEND_F = ifelse(GEND == 2, 1, 0)) %>% #create binary for female drivers
  mutate(TOLL = ifelse(TOLL == 1, 1, 0)) %>% #create binary for tolls
  mutate(PARKU_H = ifelse(PARKU == 1, 1, 0)) %>% #create binary for hourly parking
  mutate(PARKU_D = ifelse(PARKU == 2, 1, 0)) %>% #create binary for daily parking
  mutate(PARKU_W = ifelse(PARKU == 3, 1, 0)) %>% #create binary for weekly parking
  mutate(PARKU_M = ifelse(PARKU == 4, 1, 0)) #create binary for monthly parking

hist(CommuteTripsPerson$GEND)
hist(CommuteTripsPerson$TOLL)
hist(CommuteTripsPerson$PARKU)
hist(CommuteTripsPerson$TRPDUR)

glimpse(CommuteTripsPerson)
```


Test variables of interest for colinearity with dependent variable (shown in descending order of cor. score)

```{r Correlations with dependent variable}
print("Cost of Using Transit")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$TRNCT, use="complete.obs", method="pearson")
print("Presence of Toll")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$TOLL, use="complete.obs", method="pearson")
print("Trip Duration")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$TRPDUR, use="complete.obs", method="pearson")
print("Toll Amount")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$TOLLA, use="complete.obs", method="pearson")
print("Daily Parking")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$PARKU_D, use="complete.obs", method="pearson")
print("Income")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$INCOM, use="complete.obs", method="pearson")
print("Gender - Female")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$GEND_F, use="complete.obs", method="pearson")
print("Gender - Male")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$GEND_M, use="complete.obs", method="pearson")
print("Monthly Parking")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$PARKU_M, use="complete.obs", method="pearson")
print("Parking Cost")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$PARKC, use="complete.obs", method="pearson")
print("Age")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$AGE, use="complete.obs", method="pearson")
print("Hourly Parking")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$PARKU_H, use="complete.obs", method="pearson")
print("Weekly Parking")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$PARKU_W, use="complete.obs", method="pearson")
```
Let's test these variables for colinearity with the dependent variable and each other using a correlation  plot.

```{r B1 correlation matrix, fig.width=7, fig.height=5, fig.width=8}
#make matrix of variables we're testing
mod_A_vars <- CommuteTripsPerson %>% 
  dplyr::select(drivealone, TOLL, TOLLA, TRPDUR, INCOM, GEND_F, GEND_M, AGE, PARKC, PARKU_H, PARKU_D, PARKU_W, PARKU_M, TRNCT)

#r clear Nans and Inf values
mod_A_vars[sapply(mod_A_vars, is.na)] <- 0
mod_A_vars[sapply(mod_A_vars, is.nan)] <- 0
mod_A_vars[sapply(mod_A_vars, is.infinite)] <- 0

#Check for NAs
any(is.na(mod_A_vars))

#compute correlation matrix
cormatrix2 <- cor(mod_A_vars) %>% 
  round(., 2)

#plot a correlogram
corrplot(cormatrix2, method = "circle", type = "lower", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```


This plot shows that the presence of a toll and toll amount exhibit correlation with each other, meaning that their inclusion in the model is mutually exclusive. Unsurprisingly, the same is true for male and female drivers. We pick the presence of a toll (`TOLL`) and female drivers (`GEND_F`) to include in our models, because they are more correlated with drive alone trips than their closely correlated pair.

This yields our final list of independent variables to include in our models:
* `GEND_F`: Female commuter
* `AGE`: Age of commuter
* `INCOM`: total 1999 annual household income 
* `TRPDUR`: trip duration
* `TOLL`: presence of toll
* `PARKC`: parking cost ($)
* `PARKU_H`: parking priced per hour
* `PARKU_D`: parking priced per day
* `PARKU_W`: parking priced per week
* `PARKU_M`: parking priced per month
* `TRNCT`: Cost of Using Transit (dollar figure)

<br>

## B.1C Develop Binomial Logit Model

We develop a binomial logit model to XX.

First, we create a baseline model, called `mod_A`, that includes all of our variables of interest.

```{r B1 mod1}
mod_A <- glm ( drivealone ~ AGE + GEND_F + TRPDUR + INCOM + TOLL + PARKC +
                PARKU_H + PARKU_D + PARKU_W + PARKU_M + TRNCT,
               data=CommuteTripsPerson, family = binomial)
print("mod_A")
summary(mod_A)
```
We see that many of the variables included in `mod_A` lack significance.

Let's try to whittle these down with automated backwards selection using `drop1`.

```{r B1 drop1}
#Use drop1 to decide which variables to remove
#LRT shows the change in deviance, so a large LRT and small p-value means we should keep the variable in the model, and a small LRT and/or large p-value means cut it
drop1(mod_A, test="Chisq")
```
Running `drop1` gives us some useful information. We see that if we drop no variables from the model, the AIC is 2698.4. Furthermore, we want to keep variables in the model that have a large LRT (suggesting XX) and small p-value (suggesting statistical significance).

We run a new model with just `GEND_F`, `TRPDUR`, `INCOM`, `TOLL`, and `TRNCT` based on these results.

```{r B1 mod2}
mod_B <- glm ( drivealone ~  GEND_F + TRPDUR + INCOM + TOLL + TRNCT,
               data=CommuteTripsPerson, family = binomial)
summary(mod_B)
```
Interestingly, in this new model, all of the variables are significant except `TOLL` and we see that the AIC has increased slightly.

Let's see what happens if we run one more model without `TOLL`.

```{r r B1 mod3}
mod_C <- glm ( drivealone ~ GEND_F + TRPDUR + INCOM + TRNCT, data=CommuteTripsPerson, family = binomial)
summary(mod_C)
```
In this third model, all of the variables are significant, but the AIC has increased slightly again.

<br>

## B.1D Interpret Model Results

**Prediction Accuracy**

```{r B1 prediction accuracy}
predC <- as.data.frame(fitted(mod_C))
predC <- rename(predC, "prob" = "fitted(mod_C)")
predC <- mutate(predC, "binary" = ifelse(prob < 0.5, 0, 1)) #if predicted probabilities are <0.5, then classify as zero, otherwise classify as 1

#append column to original data frame
CommuteTripsPerson$binary <- predC$binary

#accuracy rate
(sum(CommuteTripsPerson$drivealone == 1 & CommuteTripsPerson$binary == 1) + 
    sum(CommuteTripsPerson$drivealone == 0 & CommuteTripsPerson$binary == 0)) /
  nrow(CommuteTripsPerson)
```
Calculating our model's prediction accuracy gives us an accuracy rate of 86% - pretty good!

Check model fit with ANOVA

```{r Check fit with ANOVA}
anova(mod_A, mod_B, mod_C, test="Chisq")
```
Our third model gives the largest decrease in deviance, suggesting it is not only our leanest, but also our meanest model.

**Interpret coefficients**
```{r interpret coeficient mod_C}
#if exp(coef) = .8:  a unit increase in X is associated with 
#a 20% decline in the odds ratio
#if exp(coef)  = 1.5:  a unit increase in X is associated with 
#a 50% increase in the odds ratio
#percent change in the odds ratio 
100 * (exp(coef(mod_C))-1)
```
The coefficients of our model suggest that:
* The odds ratio of a person driving alone to work is 40.4% *lower* if they are female.  
* a unit increase in trip duration *decreases* the odds ratio of someone driving alone to work by 0.6%.  
* a unit increase in income *increases* the odds ratio of a person driving alone to work by 5.5%.  
* And a unit increase in transit cost decreases the odds ratio of a person driving alone to work by 1.6%.  

The charts below show that these patterns hold true across the other variables.


```{r mod plots}
mod_gg1 <- data.frame(matrix(ncol = 4, nrow = nrow(CommuteTripsPerson)))
colnames(mod_gg1) <- c("TRPDUR", "INCOM", "GEND_F", "TRNCT")
mod_gg1$TRPDUR <- CommuteTripsPerson$TRPDUR
mod_gg1$INCOM <- mean(CommuteTripsPerson$INCOM)
mod_gg1$GEND_F <- 1
mod_gg1$TRNCT <- mean(CommuteTripsPerson$TRNCT)

mod_gg2 <- data.frame(matrix(ncol = 4, nrow = nrow(CommuteTripsPerson)))
colnames(mod_gg2) <- c("TRPDUR", "INCOM", "GEND_F", "TRNCT")
mod_gg2$TRPDUR <- CommuteTripsPerson$TRPDUR
mod_gg2$INCOM <- mean(CommuteTripsPerson$INCOM)
mod_gg2$GEND_F <- 0
mod_gg2$TRNCT <- mean(CommuteTripsPerson$TRNCT)

#Predict results based on models
pred_dat <- data.frame(matrix(ncol = 3, nrow = nrow(CommuteTripsPerson)))
colnames(pred_dat) <- c("TRPDUR", "Pred_GendF_Inc50K_TRNCT7", "Pred_GendM_Inc50K_TRNCT7")
pred_dat$TRPDUR <- CommuteTripsPerson$TRPDUR
#store predicted probabilities in new column
pred_dat$Pred_GendF_Inc50K_TRNCT7 <- predict(mod_C, mod_gg1, type="response")
pred_dat$Pred_GendM_Inc50K_TRNCT7 <- predict(mod_C, mod_gg2, type="response")

pred_dat_wd <- pred_dat %>% 
  pivot_longer(!TRPDUR, names_to = "Scenario")

ggplot(pred_dat_wd, aes(x = TRPDUR, y = value, colour = Scenario)) + 
        geom_line() + ylim(0,1) +
        xlab("Trip Duration") + ylab("Predicted Probability of Drive Alone") +
  theme_bw()
```

>Summarize your model results in a paragraph.

In summary, the model suggests XX.


<br>

# B.2 Walking or Biking to Work vs. Using Other Modes
>Develop a second binomial LOGIT model explaining whether a commuter
walked/biked to work or used another mode. Use any and all independent variables
you think appropriate, including dummy variables.

## B.2A Data Exploration and Processing

```{r B.2 trip data cleaning}
#grab anyone who walked or biked to work on their first one-way work trip of the day
trip_dat$walkbike[(trip_dat$TRAN1==11 | trip_dat$TRAN2==11 | 
                       trip_dat$TRAN3==11| trip_dat$TRAN4==11 |
                     trip_dat$TRAN1==14 | trip_dat$TRAN2==14 | 
                       trip_dat$TRAN3==14| trip_dat$TRAN4==14 )] <-1
trip_dat$walkbike[which(is.na(trip_dat$walkbike))] <-0 #Remove NAs

#now let's pick out that first work trip
CommuteTripsClean <- trip_dat[which(trip_dat$Dest_PTYE == 2), ]

#let's merge in on person number and SAMPN
#we have to merge together the SAMPN and the person number 
#There is an X variable in the data frame already.
#We can replicate this column by concatenating SAMPB and PERNO
CommuteTripsClean$SAMPN_PER <- paste0(CommuteTripsClean$SAMPN, CommuteTripsClean$PERNO)

#grab just the first work trip
CommuteTripsClean <- subset(CommuteTripsClean, !duplicated(SAMPN_PER))

#Retain just the columns we need from the trip data. 
CommuteTrips_walkbike <- CommuteTripsClean %>% 
  dplyr::select(SAMPN, PERNO, SAMPN_PER, TOLLA, TOLL, PARKC, PARKU, TRPDUR, walkbike)

#remove trips with a duration of zero
CommuteTrips_walkbike <- CommuteTrips_walkbike %>% 
  filter(TRPDUR != 0)

glimpse(CommuteTrips_walkbike)
summary(CommuteTrips_walkbike)

hist(CommuteTrips_walkbike$walkbike)
```

```{r merge CommuteTripsClean3 with per_hh_clean}
#Merge CommuteTrips_walkbike with per_hh_clean via SAMPN_PER
CommuteTrips_walkbike <- merge(CommuteTrips_walkbike, 
                            per_hh_clean,
                            by.x = "SAMPN_PER", 
                            by.y = "SAMPN_PER", 
                            all.x = TRUE, 
                            all.y = FALSE, 
                            sort = FALSE)
```

## B.2B Variable Selection

The binomial Dependent Variable for our model is `walkbike`: Which is whether the person walked or cycled to work vs. got to work any other way.

The values for `GEND`, `TOLL`, and `PARKU` are categorical, so we need to turn them into binary dummy variables to be able to use them in our models.

```{r B2 Dummy Vars}
#Create dummy variables from GEND, TOLL, and PARKU
CommuteTrips_walkbike <- CommuteTrips_walkbike %>% 
  mutate(GEND_M = ifelse(GEND == 1, 1, 0)) %>% #create binary for male drivers
  mutate(GEND_F = ifelse(GEND == 2, 1, 0)) %>% #create binary for female drivers
  mutate(TOLL = ifelse(TOLL == 1, 1, 0)) %>% #create binary for tolls
  mutate(PARKU_H = ifelse(PARKU == 1, 1, 0)) %>% #create binary for hourly parking
  mutate(PARKU_D = ifelse(PARKU == 2, 1, 0)) %>% #create binary for daily parking
  mutate(PARKU_W = ifelse(PARKU == 3, 1, 0)) %>% #create binary for weekly parking
  mutate(PARKU_M = ifelse(PARKU == 4, 1, 0)) #create binary for monthly parking
```

Independent variables of interest:
* `GEND`: Person X - Gender
* `AGE`: Person X - Age 
* `INCOM`: total 1999 annual household income 
* `TRPDUR`: trip duration
* `TOLL`: presence of toll (Y/N)
* `TOLLA`: toll amount
* `PARKC`: parking cost ($)
* `PARKU`: parking cost unit (per hour, day, week, month, or other)
* `TRNCT`: Cost of Using Transit (dollar figure)

```{r walk bike vars review}
summary(CommuteTrips_walkbike)
```
Remove some outliers and NAs

```{r rm outliers and NAs}
CommuteTrips_walkbike <-CommuteTrips_walkbike[which(CommuteTrips_walkbike$AGE %in% 1:110),]
CommuteTrips_walkbike <-CommuteTrips_walkbike[which(CommuteTrips_walkbike$TRPDUR %in% 0:400),]

CommuteTrips_walkbike$W1CPK[which(is.na(CommuteTrips_walkbike$W1CPK))] <-0
CommuteTrips_walkbike$PARKU_H[which(is.na(CommuteTrips_walkbike$PARKU_H))] <-0
CommuteTrips_walkbike$PARKU_D[which(is.na(CommuteTrips_walkbike$PARKU_D))] <-0
CommuteTrips_walkbike$PARKU_W[which(is.na(CommuteTrips_walkbike$PARKU_W))] <-0
CommuteTrips_walkbike$PARKU_M[which(is.na(CommuteTrips_walkbike$PARKU_M))] <-0

CommuteTrips_walkbike[sapply(CommuteTrips_walkbike, is.na)] <- 0
CommuteTrips_walkbike[sapply(CommuteTrips_walkbike, is.nan)] <- 0
CommuteTrips_walkbike[sapply(CommuteTrips_walkbike, is.infinite)] <- 0
```

Test variables of interest for colinearity with dependent variable (shown in descending order of cor. score).

```{r Correlations with dependent variable walkbike}
print("Cost of Using Transit")
cor(CommuteTrips_walkbike$walkbike, CommuteTrips_walkbike$TRNCT, use="complete.obs", method="pearson")
print("Cost to Park at Work")
cor(CommuteTrips_walkbike$walkbike, CommuteTrips_walkbike$W1CPK, use="complete.obs", method="pearson")
print("Age")
cor(CommuteTrips_walkbike$walkbike, CommuteTrips_walkbike$AGE, use="complete.obs", method="pearson")
print("Trip Duration")
cor(CommuteTrips_walkbike$walkbike, CommuteTrips_walkbike$TRPDUR, use="complete.obs", method="pearson")
print("Daily Parking")
cor(CommuteTrips_walkbike$walkbike, CommuteTrips_walkbike$PARKU_D, use="complete.obs", method="pearson")
print("Income")
cor(CommuteTrips_walkbike$walkbike, CommuteTrips_walkbike$INCOM, use="complete.obs", method="pearson")
print("Presence of Toll")
cor(CommuteTrips_walkbike$walkbike, CommuteTrips_walkbike$TOLL, use="complete.obs", method="pearson")
print("Monthly Parking")
cor(CommuteTrips_walkbike$walkbike, CommuteTrips_walkbike$PARKU_M, use="complete.obs", method="pearson")
print("Hourly Parking")
cor(CommuteTrips_walkbike$walkbike, CommuteTrips_walkbike$PARKU_H, use="complete.obs", method="pearson")
print("Weekly Parking")
cor(CommuteTrips_walkbike$walkbike, CommuteTrips_walkbike$PARKU_W, use="complete.obs", method="pearson")
print("Gender - Female")
cor(CommuteTrips_walkbike$walkbike, CommuteTrips_walkbike$GEND_F, use="complete.obs", method="pearson")
print("Gender - Male")
cor(CommuteTrips_walkbike$walkbike, CommuteTrips_walkbike$GEND_M, use="complete.obs", method="pearson")
print("Parking Cost")
cor(CommuteTrips_walkbike$walkbike, CommuteTrips_walkbike$PARKC, use="complete.obs", method="pearson")
```
Overall, there doesn't seem to be a lot of correlation between these variables and walking or biking to work, especially when exdluding the cost of using transit.

Let's test these variables for colinearity with the dependent variable and each other using a correlation  plot.

```{r correlation matrix 2, fig.width=7, fig.height=5, fig.width=8}
#make matrix of variables we're testing
mod_Aa_vars <- CommuteTrips_walkbike %>% 
  dplyr::select(walkbike, TOLL, TRPDUR, INCOM, GEND_F, GEND_M, AGE, PARKC, PARKU_H, PARKU_D, PARKU_W, PARKU_M, TRNCT, W1CPK)

#r clear Nans and Inf values
mod_Aa_vars[sapply(mod_Aa_vars, is.na)] <- 0
mod_Aa_vars[sapply(mod_Aa_vars, is.nan)] <- 0
mod_Aa_vars[sapply(mod_Aa_vars, is.infinite)] <- 0

any(is.na(mod_Aa_vars))

#compute correlation matrix
cormatrix3 <- cor(mod_Aa_vars) %>% 
  round(., 2)

#plot a correlogram
corrplot(cormatrix3, method = "circle", type = "lower", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

## B.2C Develop Binomial Logit Model

First, we create a baseline model, called `mod_Aa`, that includes all of our variables of interest.

```{r B2 mod1}
mod_Aa <- glm ( walkbike ~ AGE + GEND_F + TRPDUR + INCOM + TOLL + PARKC +
                PARKU_H + PARKU_D + PARKU_W + PARKU_M + TRNCT + W1CPK,
               data=CommuteTrips_walkbike, family = binomial)
print("mod_Aa")
summary(mod_Aa)
```

We see that many of the variables included in `mod_Aa` lack significance.

Let's try to whittle these down with automated backwards selection using `drop1`.

```{r B2 drop1}
#Use drop1 to decide which variables to remove
#LRT shows the change in deviance, so a large LRT and small p-value means we should keep the variable in the model, and a small LRT and/or large p-value means cut it
drop1(mod_Aa, test="Chisq")
```

Running `drop1` gives us some useful information. We see that if we drop no variables from the model, the AIC is 1387.8. Furthermore, we want to keep variables in the model that have a large LRT and small p-value (suggesting statistical significance).

We run a new model with just `AGE`, `TRPDUR`, `INCOM`, `TOLL`, `PARKC`, `PARKU_D`, `TRNCT`, and `W1CPK` based on these results.

```{r B2 mod2}
mod_Bb <- glm (walkbike ~  AGE + TRPDUR + INCOM + TOLL + PARKC + PARKU_D + TRNCT + W1CPK,
               data=CommuteTrips_walkbike, family = binomial)
summary(mod_Bb)
```
Good news, We see that the AIC has gone down slightly and just two variables, `TOLL` and `PARKC` appear to not be significant. So we run a new model without them.

```{r B2 mod3}
mod_Cc <- glm (walkbike ~  AGE + TRPDUR + INCOM + PARKU_D + TRNCT + W1CPK,
               data=CommuteTrips_walkbike, family = binomial)
summary(mod_Cc)
```
This raises the AIC slightly but now all of the variables are significant. It looks like neither `PARKU_D` or `W1CPK` are not very significant though, so let's see what happens when we drop those variables.

```{r B2 mod4}
mod_Dd <- glm (walkbike ~  AGE + TRPDUR + INCOM + TRNCT,
               data=CommuteTrips_walkbike, family = binomial)
summary(mod_Dd)
```
This raised the AIC by less than 1 and now all of the variables in our model are significant. Therefore, our fourth model appears to give us comparable results with the fewest variables.

<br>

## B.2D Interpret Model Results

To confirm our selection of the fourth model as the best one, we compare the fit of all four models with ANOVA.

```{r Check fit wANOVA}
anova(mod_Aa, mod_Bb, mod_Cc, mod_Dd, test="Chisq")
```
This test shows that our third model actually gives the largest decrease in deviance, suggesting that we lose a certain level of significance by dropping `PARKU_D` and `W1CPK`.

Therefore, we select our third model as our final model, instead of our fourth.


**Prediction Accuracy**

```{r B2 prediction accuracy}
predW <- as.data.frame(fitted(mod_Cc))
predW <- rename(predW, "prob" = "fitted(mod_Cc)")
predW <- mutate(predW, "binary" = ifelse(prob < 0.5, 0, 1)) #if predicted probabilities are <0.5, then classify as zero, otherwise classify as 1

#append column to original data frame
CommuteTrips_walkbike$binary <- predW$binary

#accuracy rate
(sum(CommuteTrips_walkbike$walkbike == 1 & CommuteTrips_walkbike$binary == 1) + 
    sum(CommuteTrips_walkbike$walkbike == 0 & CommuteTrips_walkbike$binary == 0)) /
  nrow(CommuteTrips_walkbike)
```
Calculating our model's prediction accuracy gives us an accuracy rate of 94% this time!

```{r false positives and negatives}
#false positives - predict that the person drove but actually didn't
print("False Positives")
sum(CommuteTrips_walkbike$walkbike == 0 & CommuteTrips_walkbike$binary == 1)

#false negatives - predict that the person didn't drive but actually did 
print("False Negatives")
sum(CommuteTrips_walkbike$walkbike == 1 & CommuteTrips_walkbike$binary == 0)
```
We see that the model seems to somewhat under predict the likelihood people will walk or bike to work.

```{r deviance}
modelChiDd <- mod_Dd$null.deviance - mod_Dd$deviance
modelChiDd
```
And we see that our final model's deviance is smaller than the null deviance, so the model fits the data better than a model with just the intercept.

**Interpret coefficients**
```{r interpret coeficient mod_Cc}
#if exp(coef) = .8:  a unit increase in X is associated with 
#a 20% decline in the odds ratio
#if exp(coef)  = 1.5:  a unit increase in X is associated with 
#a 50% increase in the odds ratio
#percent change in the odds ratio 
100 * (exp(coef(mod_Cc))-1)
```
The coefficients of our model suggest that:
* A unit increase in a person's age *decreases* the odds ratio that they will walk or bike to work by 1.6%.
* A unit increase in trip duration also *decreases* the odds ratio of a person walking or biking to work by 1.6%.  
* A a unit increase in a person's income *decreases* the odds ratio that they will walk or bike to work by 6.6%.   
* The odds ratio of a person walking or biking to work *increases* by 133.7% if they pay for parking by day as opposed to by hour, week, or month.
* A unit increase in transit cost *increases* the odds ratio of a person walking or biking to work by 1.4%.
* A unit increase in the cost of parking a vehicle at work *increases* the odds ratio of a person walking or biking to work by 0.2%.

It is important to note that based on our models, that age, trip duration, transit cost, and income seem to be the most significant factors related to whether a person walks or bikes to work.

The charts below show that these patterns hold true across the other variables.


```{r mod plots 2}
mod_gg3 <- data.frame(matrix(ncol = 6, nrow = nrow(CommuteTrips_walkbike)))
colnames(mod_gg3) <- c("TRPDUR", "AGE", "INCOM", "PARKU_D", "TRNCT", "W1CPK")
mod_gg3$TRPDUR <- mean(CommuteTrips_walkbike$TRPDUR)
mod_gg3$INCOM <- CommuteTrips_walkbike$INCOM
mod_gg3$AGE <- mean(CommuteTrips_walkbike$AGE)
mod_gg3$TRNCT <- mean(CommuteTrips_walkbike$TRNCT)
mod_gg3$PARKU_D <- 1
mod_gg3$W1CPK <- mean(CommuteTrips_walkbike$W1CPK)

mod_gg4 <- data.frame(matrix(ncol = 6, nrow = nrow(CommuteTrips_walkbike)))
colnames(mod_gg4) <- c("TRPDUR", "AGE", "INCOM", "PARKU_D", "TRNCT", "W1CPK")
mod_gg4$TRPDUR <- mean(CommuteTrips_walkbike$TRPDUR)
mod_gg4$INCOM <- CommuteTrips_walkbike$INCOM
mod_gg4$AGE <- mean(CommuteTrips_walkbike$AGE)
mod_gg4$TRNCT <- mean(CommuteTrips_walkbike$TRNCT)
mod_gg4$PARKU_D <- 0
mod_gg4$W1CPK <- mean(CommuteTrips_walkbike$W1CPK)

#Predict results based on models
pred_dat2 <- data.frame(matrix(ncol = 3, nrow = nrow(CommuteTrips_walkbike)))
colnames(pred_dat2) <- c("INCOM", "Pred_PARKU_D_Y", "Pred_PARKU_D_N")
pred_dat2$INCOM <- CommuteTrips_walkbike$INCOM
#store predicted probabilities in new column
pred_dat2$Pred_PARKU_D_Y <- predict(mod_Cc, mod_gg3, type="response")
pred_dat2$Pred_PARKU_D_N <- predict(mod_Cc, mod_gg4, type="response")

pred_dat_wd2 <- pred_dat2 %>% 
  pivot_longer(!INCOM, names_to = "Scenario")

ggplot(pred_dat_wd2, aes(x = INCOM, y = value, colour = Scenario)) + 
        geom_line() + ylim(0,1) +
        xlab("Income") + ylab("Predicted Probability of Walking or Biking") +
  theme_bw()
```

>Summarize your model results in a few paragraphs. Include a few plots or tables
that show the probabilities of each choice versus changes in key variables in your
model.

>How does this model compare to the results of the models from question B1?
Venture some reasons for the differences.

When we compare factors related to people driving alone to work from part B1 and factors related to people walking or biking to work (part B2), we observe a couple of key differences. First, gender appears to be a significant factor related to driving alone, but an insignificant one when when it comes to the decision to walk or bike to work. Additionally, paying for parking by day and the cost of parking a vehicle at work both appear to be significantly related to the probability of someone walking or biking to work, but not significantly related to the probability of them driving alone.

Possible explanations for why women are less likely to drive alone to work than men are the larger burden of childcare that women still carry in the U.S. Additionally, our models suggest that higher income is also correlated with driving alone so the gender pay gap could also be a factor.

The cost of parking at work and the parking cost unit could be associated with increased probability of walking or cycling to work because they act as deterrents. Higher cost of parking at work could decrease the perceived hardship of walking or cycling. Paying for parking by day is a surprising variable to see in this list. It could be that paying for parking more frequently creates more opportunities for a commuter to re-evaluate their mode choice.

Trip duration, income, and transit cost appear to influence the probability both of someone driving alone to work and of someone walking or cycling.

<br>

# B.3 Multi-nomial LOGIT
>Develop the best (“leanest and meanest” and unbiased) multi-nomial LOGIT model
you can explaining whether a traveler drove to work, carpooled, took public transit,
or walked or biked. Use any and all independent variables you think appropriate,
including dummy variables.

## Data Exploration and Processing
```{r MNL Setup}
rm(list=ls())

per_dat <- read_csv("https://raw.githubusercontent.com/c-townsley/pbn_a3_repo/main/Data/HH%20Travel%20Survey/per_pub.csv")

trip_dat <- read_csv("https://raw.githubusercontent.com/c-townsley/pbn_a3_repo/main/Data/HH%20Travel%20Survey/trip_pub.csv")

hh_dat <- read_csv("https://raw.githubusercontent.com/c-townsley/pbn_a3_repo/main/Data/HH%20Travel%20Survey/hh_pub_CSV.csv", col_names = TRUE)
```

##Add columns for bike, carpool, transit, or drive alone trips
```{r MNL bucket creation}
#drove to work (21), carpooled (22, 31), took public transit (see below), or walked, wheelchair, biked (11, 12, 14) 

#bike/non-motorized
trip_dat$bike[trip_dat$TRAN1==14 & is.na(trip_dat$TRAN2)]<-1
trip_dat$bike[trip_dat$TRAN1==14 & trip_dat$TRAN2==11]<-1
trip_dat$bike[trip_dat$TRAN1==14 & trip_dat$TRAN2==12]<-1

trip_dat$bike[trip_dat$TRAN1==11 & is.na(trip_dat$TRAN2)]<-1
trip_dat$bike[trip_dat$TRAN1==11 & trip_dat$TRAN2==14]<-1
trip_dat$bike[trip_dat$TRAN1==11 & trip_dat$TRAN2==12]<-1

trip_dat$bike[trip_dat$TRAN1==12 & is.na(trip_dat$TRAN2)]<-1
trip_dat$bike[trip_dat$TRAN1==12 & trip_dat$TRAN2==14]<-1
trip_dat$bike[trip_dat$TRAN1==12 & trip_dat$TRAN2==11]<-1

#drive alone
#drove to work (21)
trip_dat$drivealone[trip_dat$TRAN1==21]<-1
summary(trip_dat$drivealone)

#assume transit is 
#47 Trolley, trolley bus
#48 Jitney
#51 Subway/elevated (Market-Frankford, Broad St., PATCO)
#52 Commuter railroad (SEPTA, NJ transit)
#41 Bus (SEPTA, NJ transit)
trip_dat$transit[(trip_dat$TRAN1==47)|(trip_dat$TRAN1==48)|(trip_dat$TRAN1==51)|(trip_dat$TRAN1==41) ]<-1
trip_dat$transit[trip_dat$TRAN2==47|trip_dat$TRAN2==48| trip_dat$TRAN2==51  |trip_dat$TRAN2==41]<-1
trip_dat$transit[trip_dat$TRAN3==47|trip_dat$TRAN3==48| trip_dat$TRAN3==51  |trip_dat$TRAN3==41]<-1
trip_dat$transit[trip_dat$TRAN4==47|trip_dat$TRAN4==48| trip_dat$TRAN4==51  |trip_dat$TRAN4==41]<-1

#carpool
#carpool (22, 31)
trip_dat$carpool[trip_dat$TRAN1==22 & is.na(trip_dat$TRAN2)]<-1
trip_dat$carpool[trip_dat$TRAN1==22 & trip_dat$TRAN2==31]<-1

trip_dat$carpool[trip_dat$TRAN1==31 & is.na(trip_dat$TRAN2)]<-1
trip_dat$carpool[trip_dat$TRAN1==31 & trip_dat$TRAN2==22]<-1

## if both are true, let's give it to transit, makes the most sense 
trip_dat$drivealone[trip_dat$transit==1 & trip_dat$drivealone==1]<-0
trip_dat$bike[trip_dat$transit==1 & trip_dat$bike==1]<-0
trip_dat$drivealone[trip_dat$carpool==1 & trip_dat$drivealone==1]<-0
trip_dat$carpool[trip_dat$carpool==1 & trip_dat$transit==1]<-0
```

Clean data
```{r transit data cleaning}
#clean and convert NAs to zero
trip_dat$drivealone[which(is.na(trip_dat$drivealone))]<-0
trip_dat$bike[which(is.na(trip_dat$bike))]<-0
trip_dat$transit[which(is.na(trip_dat$transit))]<-0
trip_dat$carpool[which(is.na(trip_dat$carpool))]<-0

##clean out any rows where they are all zero 
trip_dat$SUMZERO<- trip_dat$transit+ trip_dat$bike+trip_dat$drivealone + trip_dat$carpool
trip_dat <- trip_dat[-which(trip_dat$SUMZERO == 0), ] 
#check
table(trip_dat$SUMZERO)

#now let's pick out that first work trip
CommuteTripsClean <- trip_dat[which(trip_dat$Dest_PTYE == 2), ] 

#combine modes into a single column so that we can plot
CommuteTripsClean$Commute[CommuteTripsClean$drivealone == 1] <- 1
CommuteTripsClean$Commute[CommuteTripsClean$carpool == 1] <- 2
CommuteTripsClean$Commute[CommuteTripsClean$transit == 1] <- 3
CommuteTripsClean$Commute[CommuteTripsClean$bike == 1] <- 4

#are there NAs?
table(is.na(CommuteTripsClean$Commute))
table(CommuteTripsClean$Commute)

#plot frequency of each mode chosen
hist(CommuteTripsClean$Commute)
```
Clean Data More
```{r clean data part 2}
#let's merge in on person number and SAMPN
#we have to merge together the SAMPN and the person number
#there is already a column called X that represents SAMPN + PERNO but this is how you would create it
CommuteTripsClean$SAMPN_PER <- do.call(paste, c(CommuteTripsClean[c("SAMPN", "PERNO")], sep = ""))

head(CommuteTripsClean)

##but still, some people take multiple work trips. Let's just grab the first work trip
CommuteTripsClean <- subset(CommuteTripsClean, !duplicated(SAMPN_PER))
head(CommuteTripsClean, 20)

#let's merge in on person number and SAMPN
per_hh_dat <- merge(per_dat, hh_dat,
                    by.x = "SAMPN", 
                    by.y = "SAMPN", 
                    all.x = TRUE, 
                    all.y=FALSE, 
                    sort = FALSE)

#get rid of columns we don't need
colnamCleaning<-c("VETMO", "W2TY", "W2TYO",  "W2TYP", "W2LOC", "W2IND", "W2INO", "W2OCC", "W2OCO", "W2DAY", "W2HOM", "W2HOO", "W2ST", "W2ET",  "W1WKD3", "W1WKD4", "W1WKE", "W1WKD1", "W1WKD2")
per_datClean<-per_hh_dat[ , -which(names(per_dat) %in% colnamCleaning)]
head(per_datClean)

## we have to merge together the SAMPN and the person number 
per_datClean$SAMPN_PER <- do.call(paste, c(per_datClean[c("SAMPN", "PERNO")], sep = ""))
head(per_datClean)

#merge trip with person and household
CommuteTripsPerson <- merge(CommuteTripsClean, per_datClean,
                            by.x = "SAMPN_PER", 
                            by.y = "SAMPN_PER", 
                            all.x = TRUE, 
                            all.y=FALSE, 
                            sort = FALSE)
dim(CommuteTripsPerson)
head(CommuteTripsPerson)

##let's check there is no one with 2 yes: 
CommuteTripsPerson$check<-CommuteTripsPerson$transit+CommuteTripsPerson$drivealone+CommuteTripsPerson$bike+CommuteTripsPerson$carpool
max(CommuteTripsPerson$check)

##finally, we can think about modeling 
#keep only useful variables
varsInterest <- c("SAMPN", "PERNO", "AGE", "GENDER", "INCOME", "SAMPN_PER", "TOLLA", "TOLL", "PARKC", "PARKU", "PRKUO", "TRPDUR", "drivealone", "bike", "transit", "carpool", "Commute")
CommuteTripsPerson<-CommuteTripsPerson[ , which(names(CommuteTripsPerson) %in% varsInterest)]
head(CommuteTripsPerson)

#getting rid of NAs in income
CommuteTripsPerson<-CommuteTripsPerson[-which(is.na(CommuteTripsPerson$INCOME)),]
head(CommuteTripsPerson)
```

Split Data to calculate trip duration and then rejoin data.
```{r}
#let's do the time calculation for each mode separately
#here I'm splitting the data set into three based on mode choice
Biking<-CommuteTripsPerson[which(CommuteTripsPerson$bike ==1),]
Drivealone<-CommuteTripsPerson[which(CommuteTripsPerson$drivealone ==1),]
Transit<-CommuteTripsPerson[which(CommuteTripsPerson$transit ==1),]
Carpool<-CommuteTripsPerson[which(CommuteTripsPerson$carpool ==1),]

#double check and see if each data set indeed only has information for one mode
table(Biking$Commute)
table(Drivealone$Commute)
table(Transit$Commute)
table(Carpool$Commute)

#distance = speed * time in minutes
Drivealone$distance<-(35/60)*Drivealone$TRPDUR
Biking$distance<-(12/60)*Biking$TRPDUR
Transit$distance<-(25/60)*Transit$TRPDUR
Carpool$distance<-(30/60)*Carpool$TRPDUR

#re-combine the three data sets 
dat<-rbind(Biking, Transit, Drivealone, Carpool)
head(dat)

#now we can calc a trip duration on every mode
#in other words, this is what the trip duration WOULD be if the person had chosen each mode
dat$time.auto <-dat$distance/35
dat$time.bike <- dat$distance/12
dat$time.transit <- dat$distance/25
dat$time.carpool <- dat$distance/30

head(dat)

#add a variable to enable us to recast the data later on
dat$mode[dat$bike == 1] <- "bike"
dat$mode[dat$transit == 1] <- "transit"
dat$mode[dat$drivealone == 1] <- "auto"
dat$mode[dat$carpool == 1] <- "carpool"
head(dat)

#the row indices are messed up because of the subsets we did
#need to fix them
rownames(dat) <- NULL 
head(dat)
```

#Model Building
## Subset Data for modeling
```{r subset data for modeling}
varsInterest <- c("AGE", "GENDER", "INCOME", "time.auto", "time.bike", "time.transit", "time.carpool", "mode")
dat<-dat[ , which(names(dat) %in% varsInterest)]
head(dat)

require(mlogit)

#let's try... 
head(dat)
datMNL <- mlogit.data(dat, shape="wide", choice="mode", varying=c(3:6))

#check it! 
head(datMNL, 20)
```

## Build Models
```{r model creation}
#let's first look at a few different model specifications
#formula specification
#(dependent variable ~ alternative specific | decision maker specific)
#intercept only
mod.int <- mlogit (mode ~ 1, data = datMNL)
summary(mod.int)

#the coefficients are associated with probability/proportion of each mode

#only alt. specific
mod <- mlogit (mode ~ time | 1, data = datMNL)
summary(mod)

#only decision maker (individual) specific
mod1 <- mlogit (mode ~ 1 | AGE + INCOME, data = datMNL)
summary(mod1)

#both alternative and individual specific
mod1 <- mlogit (mode ~ time | INCOME, data = datMNL)
summary(mod1)
```

## Model Testing
```{r Model Testing}
#goodness of fit
#McFadden's R squared measures how the current model 
#compares with the intercept only model
#what's the McFadden's R squared for the intercept only model?
summary(mod.int)

#likelihood ratio test (chisq test)- tests decrease in unexpained variance 
#from baseline model to final model (models must be nested!!!)
#deviance = -2 * log likelihood
#remember deviance?

#think back to chisq test we did for binomial
modelChi <- (-2*(-1104.5)) - (-2*(-1007)) 

#this is the chisq in output
summary(mod)

#calculating change in degree of freedom
mod$logLik
df.mod <- 3
mod.int$logLik
df.mod.int <- 2

chidf <- df.mod - df.mod.int

#if prob < 0.05, then the final model significantly improves fit
chisq.prob <- 1 - pchisq(modelChi, chidf)
chisq.prob #this is the chisq p-value in the output
summary(mod)

#Akaike Info Criterion
AIC(mod.int, mod, mod1)

#interpretation for coefficients
summary(mod1)

library(stargazer)
stargazer(mod.int, mod, mod1, type = "text")
```
From this, it looks like mod1 is our best model since it has the lowest AIC. But what do the results of our model mean?

Visualize the probabilities of each choice
```{r predict probs}
# Create a data frame with a range of time and income values to predict probabilities
new_data <- expand.grid(
  time = seq(min(datMNL$time), max(datMNL$time), length.out = 100),
  INCOME = c(min(datMNL$INCOME), median(datMNL$INCOME), max(datMNL$INCOME))
)

# Calculate the predicted probabilities for each transportation choice
predicted_probs <- as.data.frame(predict(mod1, newdata = new_data, type = "prob"))

# Join the predicted probabilities to the new_data data frame
new_data <- cbind(new_data, predicted_probs)

```

This plot shows our model's prediction of the impact of time on mode choice for different income levels
```{r visualize probabilities}
new_data_long <- new_data %>%
  gather(key = "mode", value = "probability", -time, -INCOME)

ggplot(new_data_long, aes(x = time, y = probability, color = mode)) +
  geom_line() +
  facet_wrap(~ INCOME, labeller = label_both) +
  labs(x = "Time", y = "Probability", title = "Probabilities of Transportation Choices vs. Time") +
  theme_minimal()

```

This plot shows our model's prediction of the impact of income on mode choice for trips below and above the median duration
```{r Transportation Mode vs Income}
# Subset the data for a few specific time values
time_subset <- c(min(datMNL$time), median(datMNL$time), max(datMNL$time))
new_data_income_subset <- new_data_long %>% filter(time %in% time_subset)

# Create the plot
ggplot(new_data_income_subset, aes(x = INCOME, y = probability, color = mode)) +
  geom_line() +
  facet_wrap(~ time, labeller = label_both) +
  labs(x = "Income", y = "Probability", title = "Probabilities of Transportation Choices vs. Income") +
  theme_minimal()
```


This model shows that as time increases, the likelihood of choosing any alternative mode of transportation decreases, with all three alternatives to driving alone (bike, carpool, and transit) having statistically significant negative coefficients. The effect of income on transportation choice is also statistically significant for carpool and transit, but not for bike. This indicates that as income increases, individuals are less likely to choose carpool or transit over driving alone, but that income has no significant impact on whether or not a given individual will bike.

(Intercept):bike, (Intercept):carpool, and (Intercept):transit: These coefficients represent the baseline log-odds of choosing each alternative mode over auto, keeping the other predictors constant. All three intercepts are negative and statistically significant, indicating that the odds of choosing bike, carpool, or transit are lower than auto when time and income are held constant.

This model has relatively low fit, suggested by its McFadden R^2 of 0.075449. Human behavior is shaped by a large number of factors, not all of which can be captured in a survey such as the one used to generate this dataset. Some additional factors to gather data on would be environmental factors such as distance to public transit and population density and personal factors such as an individual's perceived safety of modes.

The chi-square value is 251.8, with a p-value < 2.22e-16, suggesting that the model's predictors (time and income) significantly improve the model's fit compared to an intercept-only model. The model's base category is "auto," and it has the highest frequency (0.863068). This indicates that the model would perform best in predicting the "auto" mode choice, as it's the most common choice in the dataset. For the opposite reasons, this model performs worst for biking, which is the least common choice in the dataset.





