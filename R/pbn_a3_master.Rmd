---
title: "Planning by Numbers Assignment 3 - CATEGORICAL DATA MODELS"
author: "Oliver Atwood & Charlie Townsley"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: flatly
    highlight: kate
    code_folding: hide
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
rm(list=ls())
```

```{r colors, include=FALSE}
#set colors
palette <- c("#a02300", "#0059c7", "#6496b4", "#E1B93C")
#red and gold are accent
#blue is chart
#dark blue is outline
```


# Part A
>Oliver

>The dataset, Chester Urban Growth
(‘Chester_Urban_Growth.csv’ in ‘Datasets’ folder on Canvas. For metadata on this csv, see
the .xlsx file) includes selected GIS-derived land cover, land use, transportation facility, and
demographic data for Chester County, PA for 1992 and 2001. Each row in the dataset
corresponds to a 500-meter raster cell location.

```{r part A packages and data, warning = FALSE, message = FALSE}
#run every session
library(tidyverse)
library(dplyr)
library(ggplot2)
library(caret)
library(MASS)
library(corrplot)
library(stargazer)
library(broom)
library(magrittr)
library(raster)
library(sp)

Dat_A <- read_csv("https://raw.githubusercontent.com/c-townsley/pbn_a3_repo/main/Data/Chester_Urban_Growth.csv")
```

<br>

## A.1
>Read the data into R. Run descriptive statistics as necessary.

```{r summarize data, warning = FALSE, message = FALSE}
summary(Dat_A)
```

<br>

## A.2
>Create a new binary (0/1) variable, [CHNG_URB] indicating those raster cells that were
either farmland or pasture land or forest in 1992, and which converted to urban uses by
2001. Indicate these cells with a 1. All other cells should be coded to 0.

```{r Land Use Change Characterization}
Dat_A <- Dat_A %>% 
  mutate(CHNG_URB = ifelse(FARM92 == 1 | PASTURE92 == 1 | FOREST92 == 1 & URBAN01 == 1, 1, 0))

```
<br>

## A.3
>Build a best “lean & mean” binomial logit model that identifies the determinants of
agricultural/pasture/forest land-to-urban land use change between 1992 and 2001 (This
is the 0/1 variable created in step 2). Use whatever independent variables you think
appropriate, and be sure to explain your logic and model development steps.

```{r Correlations with Land Use Change}
print("SLOPE")
cor(Dat_A$CHNG_URB, Dat_A$SLOPE, use="complete.obs", method="pearson")
print("FOURLNE300")
cor(Dat_A$CHNG_URB, Dat_A$FOURLNE300, use="complete.obs", method="pearson")
print("INTERST800")
cor(Dat_A$CHNG_URB, Dat_A$INTERST800, use="complete.obs", method="pearson")
print("REGRAIL300")
cor(Dat_A$CHNG_URB, Dat_A$REGRAIL300, use="complete.obs", method="pearson")
print("PARKS500M")
cor(Dat_A$CHNG_URB, Dat_A$PARKS500M, use="complete.obs", method="pearson")
print("WATER100")
cor(Dat_A$CHNG_URB, Dat_A$WATER100, use="complete.obs", method="pearson")
print("CITBORO_10")
cor(Dat_A$CHNG_URB, Dat_A$CITBORO_10, use="complete.obs", method="pearson")
print("DIST_WATER")
cor(Dat_A$CHNG_URB, Dat_A$DIST_WATER, use="complete.obs", method="pearson")
print("DIST_RAILS")
cor(Dat_A$CHNG_URB, Dat_A$DIST_RAILS, use="complete.obs", method="pearson")
print("DIST_REGRA")
cor(Dat_A$CHNG_URB, Dat_A$DIST_REGRA, use="complete.obs", method="pearson")
print("DIST_PASSR")
cor(Dat_A$CHNG_URB, Dat_A$DIST_PASSR, use="complete.obs", method="pearson")
print("DIST_4LNE_")
cor(Dat_A$CHNG_URB, Dat_A$DIST_4LNE_, use="complete.obs", method="pearson")
print("DIST_INTER")
cor(Dat_A$CHNG_URB, Dat_A$DIST_INTER, use="complete.obs", method="pearson")
print("DIST_PARKS")
cor(Dat_A$CHNG_URB, Dat_A$DIST_PARKS, use="complete.obs", method="pearson")
print("PAL_WETLND")
cor(Dat_A$CHNG_URB, Dat_A$PAL_WETLND, use="complete.obs", method="pearson")
print("POPDEN90")
cor(Dat_A$CHNG_URB, Dat_A$POPDEN90, use="complete.obs", method="pearson")
print("MEDINC90")
cor(Dat_A$CHNG_URB, Dat_A$MEDINC90, use="complete.obs", method="pearson")
print("MEDHSEVAL_")
cor(Dat_A$CHNG_URB, Dat_A$MEDHSEVAL_, use="complete.obs", method="pearson")
print("PCT_WHITE_")
cor(Dat_A$CHNG_URB, Dat_A$PCT_WHITE_, use="complete.obs", method="pearson")
print("PCT_SFHOME")
cor(Dat_A$CHNG_URB, Dat_A$PCT_SFHOME, use="complete.obs", method="pearson")
print("PCT_POV_90")
cor(Dat_A$CHNG_URB, Dat_A$PCT_POV_90, use="complete.obs", method="pearson")
print("PCT_HSB_19")
cor(Dat_A$CHNG_URB, Dat_A$PCT_HSB_19, use="complete.obs", method="pearson")
print("PCT_COLGRD")
cor(Dat_A$CHNG_URB, Dat_A$PCT_COLGRD, use="complete.obs", method="pearson")
```
The variables in order of correlation with CHNG_URB are as follows:
"DIST_INTER" 0.3188977
"DIST_REGRA" 0.2908341
"DIST_RAILS" 0.290569
"SLOPE" -0.2689929
"DIST_PARKS" 0.2504532
"REGRAIL300" -0.2117686
"INTERST800" -0.2075421
"MEDHSEVAL_" -0.1595283
"MEDINC90" -0.1479734
"PCT_POV_90" 0.115068
"PCT_COLGRD" 0.115068
"DIST_WATER" -0.09334977
"PCT_WHITE_" -0.08178655
"DIST_PASSR" 0.0765004
"PARKS500M" -0.07496059
"PCT_HSB_19" 0.07437706
"POPDEN90" -0.07412875
"DIST_4LNE_" 0.07289739
"FOURLNE300" -0.05593594
"PCT_SFHOME" -0.0524793
"CITBORO_10" -0.03061511
"WATER100" -0.01852956
"PAL_WETLND" -0.007407224

Let's test the top 10 variables mos correlated with CHNG_URB for colinearity using a cor plot.
```{r correlation matrix, fig.width=7, fig.height=5, fig.width=8}
#make matrix of variables we're testing
mod_vars <- Dat_A %>% 
  dplyr::select(CHNG_URB, DIST_INTER, DIST_REGRA, DIST_RAILS, SLOPE, DIST_PARKS, REGRAIL300, INTERST800, MEDHSEVAL_, MEDINC90, PCT_POV_90)

#compute correlation matrix
cormatrix <- cor(mod_vars) %>% 
  round(., 2)

#plot a correlogram
corrplot(cormatrix, method = "circle", type = "lower", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

This plot shows that the following variables exhibit correlation with each other, meaning that their inclusion in the model is mutually exclusive.

DIST_INTER & DIST_REGRA & DIST_RAILS
DIST_INTER & INTERST800
MEDHSEVAL_ & MEDINC09

```{r Binomial Logit Model 1}
mod1 <- glm (CHNG_URB ~ DIST_REGRA + SLOPE + DIST_PARKS + REGRAIL300 + INTERST800 + MEDHSEVAL_ + PCT_POV_90, data=Dat_A, family = binomial)
summary(mod1)
```
PCT_POV_90 appears to be insignificant, so let's try removing it from model 2
```{r Binomial Logit Model 2}
mod2<- glm (CHNG_URB ~ DIST_REGRA + SLOPE + DIST_PARKS + REGRAIL300 + INTERST800 + MEDHSEVAL_, data=Dat_A, family = binomial)
summary(mod2)
```
When we remove PCT_POV_90, the AIC decreases by 0.2. INTERST800 also appears to be insignificant, so what happens if we remove that too?

```{r Binomial Logit Model 3}
mod3<- glm (CHNG_URB ~ DIST_REGRA + SLOPE + DIST_PARKS + REGRAIL300 + MEDHSEVAL_, data=Dat_A, family = binomial)
summary(mod3)
```
Now the AIC is back to where it was in the first model. Also, the residual deviance is 0.2 higher than model 2.

Since DIST_INTER is colinear with DIST_REGRA, DIST_RAILS, and INTERST800, it was not included in the first version of the model, so let's try a model with DIST_INTER.

```{r Binomial Logit Model 4}
mod4<- glm (CHNG_URB ~ DIST_INTER + SLOPE + DIST_PARKS + REGRAIL300 + MEDHSEVAL_, data=Dat_A, family = binomial)
summary(mod4)
```
The AIC increased on this model compared to the previous ones, so it seems like model 2 is the way to go.

```{r Calculate Prediction Accuracy}
#model1
#calc the predicted probabilities based on the model
pred <- as.data.frame(fitted(mod1))
pred <- rename(pred, "prob" = "fitted(mod1)")
pred <- mutate(pred, "binary" = ifelse(prob < 0.5, 0, 1))
#append column to original data frame
Dat_A$binary <- pred$binary
head(Dat_A)
#calculate accuracy rate
mod1_Acc <- (sum(Dat_A$URBAN01 == 1 & Dat_A$binary == 1) + sum(Dat_A$URBAN01 == 0 & Dat_A$binary == 0)) / nrow(Dat_A)

#model2
#calc the predicted probabilities based on the model
pred <- as.data.frame(fitted(mod2))
pred <- rename(pred, "prob" = "fitted(mod2)")
pred <- mutate(pred, "binary" = ifelse(prob < 0.5, 0, 1))
#append column to original data frame
Dat_A$binary <- pred$binary
head(Dat_A)
#calculate accuracy rate
mod2_Acc <- (sum(Dat_A$URBAN01 == 1 & Dat_A$binary == 1) + sum(Dat_A$URBAN01 == 0 & Dat_A$binary == 0)) / nrow(Dat_A)

#model3 
#calc the predicted probabilities based on the model
pred <- as.data.frame(fitted(mod3))
pred <- rename(pred, "prob" = "fitted(mod3)")
pred <- mutate(pred, "binary" = ifelse(prob < 0.5, 0, 1))
#append column to original data frame
Dat_A$binary <- pred$binary
head(Dat_A)
#calculate accuracy rate
mod3_Acc <- (sum(Dat_A$URBAN01 == 1 & Dat_A$binary == 1) + sum(Dat_A$URBAN01 == 0 & Dat_A$binary == 0)) / nrow(Dat_A)

#model4
#calc the predicted probabilities based on the model
pred <- as.data.frame(fitted(mod4))
pred <- rename(pred, "prob" = "fitted(mod4)")
pred <- mutate(pred, "binary" = ifelse(prob < 0.5, 0, 1))
#append column to original data frame
Dat_A$binary <- pred$binary
head(Dat_A)
#calculate accuracy rate
mod4_Acc <- (sum(Dat_A$URBAN01 == 1 & Dat_A$binary == 1) + sum(Dat_A$URBAN01 == 0 & Dat_A$binary == 0)) / nrow(Dat_A)

print("model 1 Accuracy")
mod1_Acc
print("model 2 Accuracy")
mod2_Acc
print("model 3 Accuracy")
mod3_Acc
print("model 4 Accuracy")
mod4_Acc
```
<br>

Let's try this another way. What happens if we let the machine do the work of sorting out which variables to include in the model?
```{r ML_Model1}
Dat_B <- Dat_A %>% dplyr::select(-CHESCO, -X, -Y, -PCT_COLGRD, -binary, -FARM92, -PASTURE92, -FOREST92, -URBAN01, -URBAN92)

set.seed(3456)
trainIndex <- createDataPartition(Dat_A$CHNG_URB, p = .70, list = FALSE, times = 1)

Dat_B_Train <- Dat_B[ trainIndex,]
Dat_B_Test  <- Dat_B[-trainIndex,]

ML_Model1 <- glm(CHNG_URB ~ ., family="binomial"(link="logit"), data = Dat_B_Train)
summary(ML_Model1)
```

Train another model, this time removing the variables we know are colinear from our corr plot.
```{r ML_Model2}
Dat_C <- Dat_B %>% dplyr::select(-DIST_REGRA, -DIST_RAILS, -INTERST800, -MEDINC90)

set.seed(3456)
trainIndex2 <- createDataPartition(Dat_C$DIST_INTER, p = .70, list = FALSE, times = 1)

Dat_C_Train <- Dat_C[ trainIndex2,]
Dat_C_Test  <- Dat_C[-trainIndex2,]

ML_Model2 <- glm(CHNG_URB ~ ., family="binomial"(link="logit"), data = Dat_C_Train)
summary(ML_Model2)
```
The AIC increased on Model2 relative to Model1...

Train a third model, removing variables with poor performance indicated by high p-values, along with the intercept.
```{r ML_Model3}
Dat_D <- Dat_C %>% dplyr::select(-CITBORO_10, -DIST_PASSR, -DIST_4LNE_, -PAL_WETLND, -PCT_WHITE_, -PCT_SFHOME)

set.seed(3456)
trainIndex3 <- createDataPartition(Dat_D$DIST_INTER, p = .70, list = FALSE, times = 1)

Dat_D_Train <- Dat_D[ trainIndex3,]
Dat_D_Test  <- Dat_D[-trainIndex3,]

ML_Model3 <- glm(CHNG_URB ~ ., family="binomial"(link="logit"), data = Dat_D_Train)
summary(ML_Model3)
```

Train a fourth model, removing low-performing variables in the previous model.
```{r ML_Model4}
Dat_E <- Dat_D %>% dplyr::select(-MEDHSEVAL_, -PCT_HSB_19)

set.seed(3456)
trainIndex4 <- createDataPartition(Dat_E$DIST_INTER, p = .70, list = FALSE, times = 1)

Dat_E_Train <- Dat_E[ trainIndex4,]
Dat_E_Test  <- Dat_E[-trainIndex4,]

ML_Model4 <- glm(CHNG_URB ~ ., family="binomial"(link="logit"), data = Dat_E_Train)
summary(ML_Model4)
```

Train a fifth model, removing the lowest-performing two variables.
```{r ML_Model5}
Dat_F <- Dat_E %>% dplyr::select(-PARKS500M, -POPDEN90)

set.seed(3456)
trainIndex5 <- createDataPartition(Dat_F$DIST_INTER, p = .70, list = FALSE, times = 1)

Dat_F_Train <- Dat_F[ trainIndex5,]
Dat_F_Test  <- Dat_F[-trainIndex5,]

ML_Model5 <- glm(CHNG_URB ~ ., family="binomial"(link="logit"), data = Dat_F_Train)
summary(ML_Model5)
```

Train a sixth model, removing the lowest-performing two variables.
```{r ML_Model6}
Dat_G <- Dat_F %>% dplyr::select(-RAILSTN100, -PCT_POV_90)

set.seed(3456)
trainIndex6 <- createDataPartition(Dat_G$DIST_INTER, p = .70, list = FALSE, times = 1)

Dat_G_Train <- Dat_G[ trainIndex6,]
Dat_G_Test  <- Dat_G[-trainIndex6,]

ML_Model6 <- glm(CHNG_URB ~ ., family="binomial"(link="logit"), data = Dat_G_Train)
summary(ML_Model6)
```
Train a seventh model, removing the lowest-performing three variables.
```{r ML_Model7}
Dat_H <- Dat_G %>% dplyr::select(-FOURLNE300, -WATER100, -DIST_WATER)

set.seed(3456)
trainIndex7 <- createDataPartition(Dat_H$DIST_INTER, p = .70, list = FALSE, times = 1)

Dat_H_Train <- Dat_H[ trainIndex7,]
Dat_H_Test  <- Dat_H[-trainIndex7,]

ML_Model7 <- glm(CHNG_URB ~ ., family="binomial"(link="logit"), data = Dat_H_Train)
summary(ML_Model7)
```

Compare Results of ALL Models

```{r confusion matricies, message = FALSE, warning = FALSE}
###Model 1
classProbs <- predict(ML_Model1, Dat_B_Test, type="response")

testProbs <- data.frame(obs = as.numeric(Dat_B_Test$CHNG_URB), pred = classProbs)

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

CM_Model1 <- caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")

Model1_Acc <- CM_Model1$overall["Accuracy"]

###Model 2
classProbs <- predict(ML_Model2, Dat_C_Test, type="response")

testProbs <- data.frame(obs = as.numeric(Dat_C_Test$CHNG_URB), pred = classProbs)

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

CM_Model2 <- caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")

Model2_Acc <- CM_Model2$overall["Accuracy"]

###Model 3
classProbs <- predict(ML_Model3, Dat_D_Test, type="response")

testProbs <- data.frame(obs = as.numeric(Dat_D_Test$CHNG_URB), pred = classProbs)

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

CM_Model3 <- caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")

Model3_Acc <- CM_Model3$overall["Accuracy"]

###Model 4
classProbs <- predict(ML_Model4, Dat_E_Test, type="response")

testProbs <- data.frame(obs = as.numeric(Dat_E_Test$CHNG_URB), pred = classProbs)

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

CM_Model4 <- caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")

Model4_Acc <- CM_Model4$overall["Accuracy"]

###Model 5
classProbs <- predict(ML_Model5, Dat_F_Test, type="response")

testProbs <- data.frame(obs = as.numeric(Dat_F_Test$CHNG_URB), pred = classProbs)

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

CM_Model5 <- caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")

Model5_Acc <- CM_Model5$overall["Accuracy"]

###Model 6
classProbs <- predict(ML_Model6, Dat_E_Test, type="response")

testProbs <- data.frame(obs = as.numeric(Dat_E_Test$CHNG_URB), pred = classProbs)

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

CM_Model6 <- caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")

Model6_Acc <- CM_Model6$overall["Accuracy"]

###Model 7
classProbs <- predict(ML_Model7, Dat_F_Test, type="response")

testProbs <- data.frame(obs = as.numeric(Dat_F_Test$CHNG_URB), pred = classProbs)

testProbs$predClass  = ifelse(testProbs$pred > .5 ,1,0)

CM_Model7 <- caret::confusionMatrix(reference = as.factor(testProbs$obs), 
                       data = as.factor(testProbs$predClass), 
                       positive = "1")

Model7_Acc <- CM_Model7$overall["Accuracy"]

###Summarize Accuracy
print('Model 1')
Model1_Acc
print('Model 2')
Model2_Acc
print('Model 3')
Model3_Acc
print('Model 4')
Model4_Acc
print('Model 5')
Model5_Acc
print('Model 6')
Model6_Acc
print('Model 7')
Model7_Acc
```
From this summary of accuracy tests, it appears that Model 1 is the most accurate, at ~68%. However, the decrease in accuracy incurred through omission of variables with each successive model generated seems to be worth it, with our 'leanest and meanest' model using only four variables to predict land conversion to urban with ~65% accuracy.


<br>

## A.4
>Explain the results of your best model in a few paragraphs for non-statisticians. How
well does your model fit the data? Which factors are most important in explaining land
use change in Chester County from 1992-2001? How do you know? Do you have any
ideas for improving the performance of your model?

Out of the 23 variables in this dataset, Model 7 uses only four variables to model land use change in Chester County from 1992-2001 with 65% accuracy. According to this model, the four most important factors in explaining land use change are Slope, Land within 300 meters of SEPTA regional rail lines, Distance to Interstate, and distance to parks.


The estimate for the SLOPE variable is -0.3121, which means that for every one-unit increase in slope (percent), the log-odds of a pixel changing to urban (i.e., CHNG_URB being 1) between 1992 and 2001 decreases by 0.3121, holding other variables constant.

To make this more interpretable, we can convert the log-odds to odds by taking the exponent of the coefficient: exp(-0.3121) ≈ 0.732

This means that for every one-unit increase in slope, the odds of a pixel changing to urban between 1992 and 2001 are approximately 73.2% of the odds for the previous slope value, keeping all other predictors constant in the model.

Put another way, all else equal, the odds of a given pixel changing to urban decrease by 26.8% for every 1% increase in slope.

The simplest interpretation of this coefficient is that an increase in slope is associated with a decreased likelihood of a pixel becoming urban.

For the rest of the values, let's keep it simple:

REGRAIL300 (binary) has a coefficient of -1.016, meaning that a cell being located within 300 meters of a SEPTA regional rail line, is associated with a decreased likelihood of a pixel becoming urban.

DIST_INTER (meters) has a coefficient of .00003284, meaning that the further a pixel is from the interstate, the more likely it is to become urban.
This is likely due to the fact that areas closest to the interstate are already urban.

DIST_PARKS (meters) has a coefficient of 0.00009018 meaning that the further a pixel is from a park, the more likely it is to become urban.
This is likely due to the same phenomenon, since parks tend to be located in urban areas and also parks themselves are unlikely to urbanize.


<br>

## A.5
>Include a few plots or tables that show the probabilities versus changes in key variables
in your model.

```{r predict and plot}
Classprobs <- predict(ML_Model7, Dat_A, type="response")

Prediction <- data.frame(obs = as.numeric(Dat_A$binary),
                            pred = Classprobs)

Prediction <- Prediction %>% 
  mutate(id = row_number())

Dat_A <- Dat_A %>% 
  mutate(id = row_number())

# Assuming 'id' is the key column
Dat_A$id <- as.character(Dat_A$id)
Prediction$id <- as.character(Prediction$id)

# Join predictions to original dataframe
Dat_Pred <- Dat_A %>%
  left_join(Prediction, by = "id", copy = TRUE)
```

```{r}
#transform dataframe with predictions into a raster
Dat_Pred <- Dat_Pred %>% 
  mutate(x = X) %>% 
  mutate(y = Y)

coordinates(Dat_Pred) <- ~x+y
gridded(Dat_Pred) <- TRUE
raster_layer <- raster(Dat_Pred)


Prediction_Raster <- rasterFromXYZ(Dat_Pred)
```
```{r}

# Remove the existing file if it exists
output_file <- "/Users/oliveratwood/Box Sync/CPLN 6750 Land Use Env Modeling/FloodModel_Assignment/R_Output/CalgaryFishnet_Err.geojson"
if (file.exists(output_file)) {file.remove(output_file)}

# Write the new file using st_write()
st_write(Dat_Pred, "/Users/oliveratwood/Box Sync/CPLN 6750 Land Use Env Modeling/FloodModel_Assignment/R_Output/CalgaryFishnet_Err.geojson")

CalgaryFishnet_Pred$PredClass = ifelse(CalgaryFishnet_Pred$pred > .5 ,1,0)

ggplot() + 
  geom_sf(data=CalgaryFishnet_Pred, aes(fill=factor(ntile(pred,5))), colour=NA) +
  geom_sf(data = CalgaryBounds, fill = "transparent", color = "red") +
  scale_fill_manual(values = c("#edf8fb","#b3cde3","#8c96c6","#8856a7","#810f7c"),
                    labels=as.character(quantile(CalgaryFishnet_Pred$pred,
                                                 c(0.1,.2,.4,.6,.8),
                                                 na.rm=T)),
                    name="Predicted\nProbabilities(%)\n(Quintile\nBreaks)") +
  labs(title="Predicted Inundated Areas Calgary") +
  mapTheme
```

<br>
<br>

# Part B
>Charlie
>How do a person’s household, demographic, and trip characteristics
affect their choice of transportation mode to work? To answer this
question, we will use data from the DVRPC HHTS.

```{r part B data, warning = FALSE, message = FALSE}

per_dat <- read_csv("https://raw.githubusercontent.com/c-townsley/pbn_a3_repo/main/Data/HH%20Travel%20Survey/per_pub.csv")

trip_dat <- read_csv("https://raw.githubusercontent.com/c-townsley/pbn_a3_repo/main/Data/HH%20Travel%20Survey/trip_pub.csv")

hh_dat <- read_csv("https://raw.githubusercontent.com/c-townsley/pbn_a3_repo/main/Data/HH%20Travel%20Survey/hh_pub_CSV.csv", col_names = TRUE)

head(per_dat)
head(trip_dat)
head(hh_dat)
```

<br>

# B.1 Driving Alone to Work vs. Other Modes

In this section, we develop a binomial LOGIT model to explain whether a commuter drove to work or used another mode.

## B.1A Data Exploration and Processing

```{r process per_dat and hh_dat}
# Develop a dataset with all relevant variables 

# Want to look at the person_dat, but also want to include income, which is in the HH table. 
# matching the datasets on HH ID number, which is the variable "SAMPN"
per_hh_dat <- merge(per_dat, hh_dat,
                    by.x = "SAMPN", #per_dat is the x
                    by.y = "SAMPN", #hh_dat is the y
                    all.x = TRUE,   #taking every row of X and finding the matching Y
                    all.y = FALSE,  #Exclude rows in hh_dat that don't have a matching value in per_dat
                    sort = FALSE)
head(per_hh_dat)

#select variables of interest
per_hh_clean <- per_hh_dat %>% 
  dplyr::select(SAMPN, PERNO, GEND, AGE, INCOM, W1CPK, W1EPK, TRNST, VTRAN, TRNCT, WSTIM, WETIM)

#combine SAMPN and PERNO
per_hh_clean$SAMPN_PER <- paste0(per_hh_clean$SAMPN, per_hh_clean$PERNO) 

#remove NAs
per_hh_clean$INCOM[which(is.na(per_hh_clean$INCOM))]<-0

summary(per_hh_clean)
glimpse(per_hh_clean)
```

```{r explore transit mode data}
head(trip_dat,10)
hist(trip_dat$TRAN1)
hist(trip_dat$TRAN2)
hist(trip_dat$TRAN3)
```


```{r trip data cleaning}
#grab anyone who drives alone to work on their first one-way work trip of the day
trip_dat$drivealone[(trip_dat$TRAN1==21 | trip_dat$TRAN2==21 | 
                       trip_dat$TRAN3==21| trip_dat$TRAN4==21)] <-1
trip_dat$drivealone[which(is.na(trip_dat$drivealone))] <-0 #Remove NAs

#now let's pick out that first work trip
CommuteTripsClean <- trip_dat[which(trip_dat$Dest_PTYE == 2), ]

#let's merge in on person number and SAMPN
#we have to merge together the SAMPN and the person number 
#There is an X variable in the data frame already.
#We can replicate this column by concatenating SAMPB and PERNO
CommuteTripsClean$SAMPN_PER <- paste0(CommuteTripsClean$SAMPN, CommuteTripsClean$PERNO)

#grab just the first work trip
CommuteTripsClean <- subset(CommuteTripsClean, !duplicated(SAMPN_PER))

#Retain just the columns we need from the trip data. 
CommuteTripsClean2 <- CommuteTripsClean %>% 
  dplyr::select(SAMPN, PERNO, SAMPN_PER, TOLLA, TOLL, PARKC, PARKU, TRPDUR, drivealone)

#remove NAs
CommuteTripsClean2$TOLLA[which(is.na(CommuteTripsClean2$TOLLA))]<-0
CommuteTripsClean2$TOLL[which(is.na(CommuteTripsClean2$TOLL))]<-0
CommuteTripsClean2$PARKU[which(is.na(CommuteTripsClean2$PARKU))]<-0
CommuteTripsClean2$PARKC[which(is.na(CommuteTripsClean2$PARKC))]<-0

glimpse(CommuteTripsClean2)
summary(CommuteTripsClean2)
```

```{r merge CommuteTripsClean2 with per_hh_clean}
#Merge CommuteTripsClean2 with per_hh_clean via SAMPN_PER
CommuteTripsPerson <- merge(CommuteTripsClean2, 
                            per_hh_clean,
                            by.x = "SAMPN_PER", 
                            by.y = "SAMPN_PER", 
                            all.x = TRUE, 
                            all.y = FALSE, 
                            sort = FALSE)
```

<br>

## B.1B Variable Selection

>Develop the best (“leanest and meanest” and unbiased) binomial LOGIT model you
can explaining whether a commuter drove to work or used another mode. Use any
and all independent variables you think appropriate, including dummy variables.

>From megan: don't include variables that have a lot of zeros
Ok to do drive alone instead of all driving. Just say how we define it

The binomial Dependent Variable for our model is `drivealone`: Which is whether the person alone drove to work vs. got to work any other way.

Independent variables of interest:
* `GEND`: Person X - Gender
* `AGE`: Person X - Age 
* `INCOM`: total 1999 annual household income 
* `TRPDUR`: trip duration
* `TOLL`: presence of toll (Y/N)
* `TOLLA`: toll amount
* `PARKC`: parking cost ($)
* `PARKU`: parking cost unit (per hour, day, week, month, or other)

> Others to maybe add later from TRIP_PUB:
SUBTR     Boarding Station
ACCESS    Access to Boarding Station
PAY1O     First Transit Payment - Other
FARE1     Fare Amount 1 
TRFR      Transfers
PAY2      Second Transit Payment 
PAY2O     Second Transit Payment - Other 
FARE2     Fare Amount 2    
PAY3      Third Transit Payment    
PAY3O     Third Transit Payment - Other 
FARE3     Fare Amount 3 
EGRESS    Egress From Exit Station
OEGRES    Other Egress 

> Other to maybe add from per_hh_dat
W1CPK     Cost to Park Vehicle at Work (dollar figure)  
W1EPK     Employer Subsidize Parking 
TRNST     Employer Subsidize Transit  
VTRAN     Use Transit Subsidy
TRNCT     Cost of Using Transit (dollar figure)
WSTIM     Work Start Time
WETIM     Work End Time 

The values for `GEND`, `TOLL`, and `PARKU` are categorical, so we need to turn them into binary dummy variables to be able to use them in our models.

```{r B1 Dummy Vars}
#Create dummy variables from GEND, TOLL, and PARKU
CommuteTripsPerson <- CommuteTripsPerson %>% 
  mutate(GEND_M = ifelse(GEND == 1, 1, 0)) %>% #create binary for male drivers
  mutate(GEND_F = ifelse(GEND == 2, 1, 0)) %>% #create binary for female drivers
  mutate(TOLL = ifelse(TOLL == 1, 1, 0)) %>% #create binary for tolls
  mutate(PARKU_H = ifelse(PARKU == 1, 1, 0)) %>% #create binary for hourly parking
  mutate(PARKU_D = ifelse(PARKU == 2, 1, 0)) %>% #create binary for daily parking
  mutate(PARKU_W = ifelse(PARKU == 3, 1, 0)) %>% #create binary for weekly parking
  mutate(PARKU_M = ifelse(PARKU == 4, 1, 0)) #create binary for monthly parking

hist(CommuteTripsPerson$GEND_M)
hist(CommuteTripsPerson$GEND_F)
hist(CommuteTripsPerson$TOLL)
hist(CommuteTripsPerson$PARKU)

```


Test variables of interest for colinearity with independent variable (shown in descending order of cor. score)

```{r Correlations with dependent variable}
print("Presence of Toll")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$TOLL, use="complete.obs", method="pearson")
print("Trip Duration")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$TRPDUR, use="complete.obs", method="pearson")
print("Toll Amount")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$TOLLA, use="complete.obs", method="pearson")
print("Daily Parking")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$PARKU_D, use="complete.obs", method="pearson")
print("Income")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$INCOM, use="complete.obs", method="pearson")
print("Gender - Female")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$GEND_F, use="complete.obs", method="pearson")
print("Gender - Male")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$GEND_M, use="complete.obs", method="pearson")
print("Monthly Parking")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$PARKU_M, use="complete.obs", method="pearson")
print("Parking Cost")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$PARKC, use="complete.obs", method="pearson")
print("Age")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$AGE, use="complete.obs", method="pearson")
print("Hourly Parking")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$PARKU_H, use="complete.obs", method="pearson")
print("Weekly Parking")
cor(CommuteTripsPerson$drivealone, CommuteTripsPerson$PARKU_W, use="complete.obs", method="pearson")
```
Let's test these variables for colinearity with the dependent variable and each other using a correlation  plot.

```{r correlation matrix 2, fig.width=7, fig.height=5, fig.width=8}
#make matrix of variables we're testing
mod_A_vars <- CommuteTripsPerson %>% 
  dplyr::select(drivealone, TOLL, TOLLA, TRPDUR, INCOM, GEND_F, GEND_M, AGE, PARKC, PARKU_H, PARKU_D, PARKU_W, PARKU_M)

#r clear Nans and Inf values
mod_A_vars[sapply(mod_A_vars, is.na)] <- 0
mod_A_vars[sapply(mod_A_vars, is.nan)] <- 0
mod_A_vars[sapply(mod_A_vars, is.infinite)] <- 0

any(is.na(mod_A_vars))

#compute correlation matrix
cormatrix2 <- cor(mod_A_vars) %>% 
  round(., 2)

#plot a correlogram
corrplot(cormatrix2, method = "circle", type = "lower", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```


This plot shows that the presence of a toll and drive alone trips exhibit correlation with each other, meaning that their inclusion in the model is mutually exclusive.

<br>

## B.1C Develop Binomial Logit Model

```{r binomial logit model 1}
mod_A <- glm ( drivealone ~ AGE+GEND+TRPDUR+INCOM+TOLLA+PARKC, data=CommuteTripsPerson, family = binomial)
print("mod_A")
summary(mod_A)

#Use drop1 to decide which variables to remove
#LRT shows the change in deviance, so a large LRT and small p-value means we should keep the variable in the model, and a small LRT and/or large p-value means cut it
drop1(mod, test="Chisq")

mod_B <- glm ( drivealone ~ AGE, data=CommuteTripsPerson, family = binomial)
print("mod_B")
summary(mod_B)

mod_C <- glm ( drivealone ~ GEND, data=CommuteTripsPerson, family = binomial)
print("mod_C")
summary(mod_C)

mod_D <- glm ( drivealone ~ GEND + TRPDUR, data=CommuteTripsPerson, family = binomial)
print("mod_D")
summary(mod_D)

mod_E <- glm ( drivealone ~ GEND + TRPDUR + INCOM, data=CommuteTripsPerson, family = binomial)
print("mod_E")
summary(mod_E)

mod_F <- glm ( drivealone ~ GEND + TRPDUR + INCOM + TOLLA, data=CommuteTripsPerson, family = binomial)
print("mod_F")
summary(mod_F)

mod_G <- glm ( drivealone ~ GEND + TRPDUR + INCOM + PARKC, data=CommuteTripsPerson, family = binomial)
print("mod_G")
summary(mod_G)

mod_H <- glm ( drivealone ~ AGE + TRPDUR + INCOM, data=CommuteTripsPerson, family = binomial)
print("mod_H")
summary(mod_H)

#mod_E seems to be the most significant one
```
Interpret model results

```{r mod plots}
# now, if you plot just the model fit using the existing data, you get:
points (CommuteTripsPerson$TRPDUR, fitted (mod_H), col="red")

####What about doing this in ggplot?
newdat_gg <- data.frame(matrix(ncol = 3, nrow = nrow(CommuteTripsPerson)))
colnames(newdat_gg) <- c("TRPDUR", "INCOM", "AGE")
newdat_gg$TRPDUR <- CommuteTripsPerson$TRPDUR
newdat_gg$INCOME <- 50000
newdat_gg$AGE <- mean(CommuteTripsPerson$AGE)

newdat_gg_1 <- data.frame(matrix(ncol = 3, nrow = nrow(CommuteTripsPerson)))
colnames(newdat_gg_1) <- c("TRPDUR", "INCOM", "AGE")
newdat_gg_1$TRPDUR <- CommuteTripsPerson$TRPDUR
newdat_gg_1$INCOME <- 50000
newdat_gg_1$AGE <- 23

newdat_gg_2 <- data.frame(matrix(ncol = 3, nrow = nrow(CommuteTripsPerson)))
colnames(newdat_gg_2)<- c("TRPDUR", "INCOM", "AGE")
newdat_gg_2$TRPDUR <- CommuteTripsPerson$TRPDUR
newdat_gg_2$INCOME <- 50000
newdat_gg_2$AGE <- 60

#Predict results based on models
pred_dat <- data.frame(matrix(ncol = 4, nrow = nrow(CommuteTripsPerson)))
colnames(pred_dat) <- c("TRPDUR", "Pred_Age43_Inc50K", "Pred_Age23_Inc50K", "Pred_Age60_Inc50K")
pred_dat$TRPDUR <- CommuteTripsPerson$TRPDUR
pred_dat$Pred_Age43_Inc50K <- predict(mod_H, newdat_gg, type="response") #store predicted probabilities in new column
pred_dat$Pred_Age23_Inc50K <- predict(mod_H, newdat_gg_1, type="response")
pred_dat$Pred_Age60_Inc50K <- predict(mod_H, newdat_gg_2, type="response")

?predict

dat <- gather(pred_dat, -TRPDUR, key = "Scenario", value = "value") #gather is old syntax for going from long to wide or vice versa (new is "pivot_longer/pivot_wider")

ggplot(dat, aes(x = TRPDUR, y = value, colour = Scenario)) + 
        geom_line() + ylim(0,1) +
        xlab("Trip Duration") + ylab("Predicted Probability of Drive Alone")
```

Prediction

```{r predict results}
#Predict results based on models
pred_dat <- data.frame(matrix(ncol = 4, nrow = nrow(CommuteTripsPerson)))
colnames(pred_dat) <- c("TRPDUR", "Pred_Age43_Inc50K", "Pred_Age23_Inc50K", "Pred_Age60_Inc50K")
pred_dat$TRPDUR <- CommuteTripsPerson$TRPDUR
pred_dat$Pred_Age43_Inc50K<- predict(mod_E, newdat_gg, type="response") #store predicted probabilities in new column
pred_dat$Pred_Age23_Inc50K<- predict(mod_E, newdat_gg_1, type="response")
pred_dat$Pred_Age60_Inc50K<- predict(mod_E, newdat_gg_2, type="response")

?predict
```

Calculate Prediction Accuracy

```{r prediction accuracy}
pred <- as.data.frame(fitted(mod))
pred <- rename(pred, "prob" = "fitted(mod)")
pred <- mutate(pred, "binary" = ifelse(prob < 0.5, 0, 1)) #if predicted probabilities are <0.5, then classify as zero, otherwise classify as 1

#append column to original data frame
CommuteTripsPerson_clean$binary <- pred$binary
head(CommuteTripsPerson_clean)

#accuracy rate - 86% - pretty high - yay!
(sum(CommuteTripsPerson_clean$drivealone == 1 & CommuteTripsPerson_clean$binary == 1) + 
    sum(CommuteTripsPerson_clean$drivealone == 0 & CommuteTripsPerson_clean$binary == 0)) /
  nrow(CommuteTripsPerson_clean)
```

Check model fit (with ANOVA?)

Interpret coefficients
```{r interpret coeficient mod_H}
#if exp(coef) = .8:  a unit increase in X is associated with 
#a 20% decline in the odds ratio#CHANGE!
#if exp(coef)  = 1.5:  a unit increase in X is associated with 
#a 50% increase in the odds ratio
#percent change in the odds ratio 
100 * (exp(coef(mod))-1)
```


>Summarize your model results in a paragraph.

>Include a few plots or tables that show the probabilities of each choice versus changes in key variables in your model.

>NOTE: THIS MAY REQUIRE YOU TO LOOK THROUGH THE META_DATA. It requires
selecting the observations from the trip table that correspond to a person’s work trip.

<br>

# B.2 Walking or Biking to Work vs. Using Other Modes
>Develop a second binomial LOGIT model explaining whether a commuter
walked/biked to work or used another mode. Use any and all independent variables
you think appropriate, including dummy variables.

>Summarize your model results in a few paragraphs. Include a few plots or tables
that show the probabilities of each choice versus changes in key variables in your
model.

>How does this model compare to the results of the models from question B1?
Venture some reasons for the differences.

<br>

# B.3 Multi-nomial LOGIT
>Develop the best (“leanest and meanest” and unbiased) multi-nomial LOGIT model
you can explaining whether a traveler drove to work, carpooled, took public transit,
or walked or biked. Use any and all independent variables you think appropriate,
including dummy variables.

>Summarize your model results in a few paragraphs. Include a few plots or tables
that show the probabilities of each choice versus changes in key variables in your
model.

>How well does your model predict observed mode choices? For which modes
does it perform best? Worst? How do you know?

>What ideas do you have for additional variables (that are not available in the
data) that might make for better models?

